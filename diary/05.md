## Mon 2 Sep 2019

Starting a new file for a new phase of this project: finalizing!

This is the plan of action:

1. Get deep-coref results
2. Finish the first complete draft of the paper
3. Give it to Piek to get any early feedback (first impressions and such)
4. Polish it with Antske
5. Submit it to TACL before the 01 Oct deadline
6. If rejected, still have some time to revise it for ACL

## Fri 6 Sep 

Working on the paper while shaping up some results as well. I got into problems
again with packages on my laptop. It was a fool of me to reinstall my python version
but maybe my project wasn't that reproducible so it's good to fix it now...

Try to pick up pipenv again...

## Sat 7 Sep

Managed to setup my local reporting environment. Wrote a few more paragraphs.

Got some results, a bit lower than expected...

    output/deep-coref/conll-2012-transformed-exported/men_40/models/reward_rescaling/best_weights.hdf5
    Evaluating model on test
    1032/1032 [==============================] - 59s - loss: 0.6102
    Writing output
    test - MUC: 48.67 - B3: 38.07 - CEAFE: 42.40 - LEA 31.84 - CoNLL 43.05
    test - loss: 0.6102 - CN: 10475 - CL: 50280 - FN: 3282 - FL: 3468 - WL: 2450
        ranking: 0.9535 - anaphoricity: 0.7563

    output/deep-coref/conll-2012-transformed-exported/orig/models/reward_rescaling/best_weights.hdf5
    Evaluating model on test
    1255/1255 [==============================] - 76s - loss: 0.5515
    Writing output
    test - MUC: 77.26 - B3: 67.27 - CEAFE: 61.51 - LEA 63.89 - CoNLL 68.68
    test - loss: 0.5515 - CN: 20284 - CL: 53197 - FN: 4260 - FL: 2741 - WL: 2153
        ranking: 0.9611 - anaphoricity: 0.8528
    82.79 & 72.43 & 77.26 & 74.71 & 61.17 & 67.27 & 66.93 & 56.90 & 61.51 & 68.68

It his higher than reported results, but I'm testing on dev+test so the numbers 
aren't comparable. I'm not sure if it used given mentions...

## Sun 8 Sep: writing and evaluating deep-coref

Submitted a evaluation job any way. First, let's finish the full round of experiments
and get the paper in shape. I'll dig into the problem later.

    [minhle@int1 EvEn]$ sbatch scripts/deep-coref/eval-multiple-python-models.job

Finished evaluation of deep-coref, added results to the paper but they're weird...

I'll need a few more weeks to finish the paper, there're some outstanding questions...
Couldn't manage to prepare a presentation for the meeting with Piek and Antske

## Mon 9 Sep: 

Had a productive meeting with Piek and Antske. Got advice on how to interpret the results.

Working on another analysis: train e2e on a corpus and eval on a different to see if
it spontaneously pick up context or needs to be trained specifically for it.

    [minhle@int2 ~]$ drake output/e2e/eval_all_cross_logs
    [minhle@int2 ~]$ !sque
    squeue | grep minh
            6819320    normal eval_all   minhle PD       0:00      1 (ReqNodeNotAvail, UnavailableNodes:gcn[1,62-63],tcn[19-36,899,1027-1028,1030-1032,1035-1037,1730-1731,1733-1735,1738-1740])

Waiting...

As discussed with Piek and Antske, I'll need to resample documents to produce some
kind of statistical significance.

- for systems: to know if something is better/worse than chance
- for humans: to correct the genre distribution

Each case would need a different technical solution because we can rerun systems but not
manual annotation.

## Tue 10 Sep: Generating re-samples

Finished development of a resampling script: `scripts/resampling/generate_resamples.py`

My job is still waiting in queue so I decided to run on login node instead. The program
only use one CPU and little memory anyway.

    [minhle@int1 EvEn]$ . scripts/setup-cartesius.sh
    [minhle@int1 EvEn]$ drake -a output/conll-2012-transformed-resampled
    Running 1 steps with concurrence of 1...

    --- 53. Running (missing output): /nfs/home2/minhle/EvEn/././output/conll-2012-transformed-resampled <- /nfs/home2/minhle/EvEn/././output/conll-2012-consolidated, /nfs/home2/minhle/EvEn/././output/conll-2012-consolidated.SUCCESS
    ...
    Reading corpus :  88%|████████▊ | 603/687 [02:07<00:17,  4.73doc/s]

Used `screen` to prepare for the long running job:

    [minhle@int1 ~]$ screen
    [detached from 20107.pts-2.int1]

## Wed 11 Sep

Login node hung. Gave up chasing the 1 Oct deadline.

## Mon 23 Sep

Back from RecSys Copenhagen. Let's pick it up again.

Couldn't find the detached screen...

    [minhle@int1 ~]$ screen -r
    There is no screen to be resumed.

But I found this in Drake's log -- so apparently the job finished successfully:

    [minhle@int1 EvEn]$ vim drake.log
    ...
    2019-09-11 03:07:22,553 INFO Running 1 steps with concurrence of 1...
    2019-09-11 03:07:22,575 INFO
    2019-09-11 03:07:22,575 INFO --- 53. Running (missing output): /nfs/home2/minhle/EvEn/././output/conll-2012-transformed-resampled <- /nfs/home2/minhle/EvEn/././output/conll-2012-consolidated, /nfs/home2/minhle/EvEn/././output/conll-2012-consolidated.SUCCESS
    2019-09-11 03:52:13,680 INFO --- 53: /nfs/home2/minhle/EvEn/././output/conll-2012-transformed-resampled <- /nfs/home2/minhle/EvEn/././output/conll-2012-consolidated, /nfs/home2/minhle/EvEn/././output/conll-2012-consolidated.SUCCESS -> done in 2691.10s
    2019-09-11 03:52:13,687 INFO Done (1 steps run).

Sampled documents are found. The same documents are sampled across transformations
but different documents across samples:

    [minhle@int1 EvEn]$ grep "begin doc" output/conll-2012-transformed-resampled/sample_24/orig/dev_test.m_gold_conll  | head
    #begin document (pt/nt/42/nt_4210); part 0010000
    #begin document (wb/eng/00/eng_0010); part 0070001
    #begin document (mz/sinorama/10/ectb_1049); part 0040002
    #begin document (nw/xinhua/01/chtb_0130); part 0000003
    #begin document (nw/xinhua/02/chtb_0229); part 0000004
    #begin document (bc/msnbc/00/msnbc_0000); part 0110005
    #begin document (bc/cnn/00/cnn_0008); part 0010006
    #begin document (mz/sinorama/10/ectb_1060); part 0000007
    #begin document (mz/sinorama/10/ectb_1049); part 0020008
    #begin document (bn/voa/01/voa_0150); part 0000009
    
    [minhle@int1 EvEn]$ grep "begin doc" output/conll-2012-transformed-resampled/sample_24/men_100/dev_test.m_gold_conll  | head
    #begin document (pt/nt/42/nt_4210); part 0010000
    #begin document (wb/eng/00/eng_0010); part 0070001
    #begin document (mz/sinorama/10/ectb_1049); part 0040002
    #begin document (nw/xinhua/01/chtb_0130); part 0000003
    #begin document (nw/xinhua/02/chtb_0229); part 0000004
    #begin document (bc/msnbc/00/msnbc_0000); part 0110005
    #begin document (bc/cnn/00/cnn_0008); part 0010006
    #begin document (mz/sinorama/10/ectb_1060); part 0000007
    #begin document (mz/sinorama/10/ectb_1049); part 0020008
    #begin document (bn/voa/01/voa_0150); part 0000009

    [minhle@int1 EvEn]$ grep "begin doc" output/conll-2012-transformed-resampled/sample_23/orig/dev_test.m_gold_conll  | head
    #begin document (bn/cnn/02/cnn_0210); part 0000000
    #begin document (nw/wsj/23/wsj_2331); part 0000001
    #begin document (bc/cnn/00/cnn_0000); part 0030002
    #begin document (wb/eng/00/eng_0000); part 0030003
    #begin document (bn/cnn/03/cnn_0339); part 0000004
    #begin document (bn/pri/00/pri_0060); part 0000005
    #begin document (wb/c2e/00/c2e_0040); part 0000006
    #begin document (bn/nbc/00/nbc_0029); part 0000007
    #begin document (tc/ch/00/ch_0030); part 0010008
    #begin document (nw/wsj/23/wsj_2381); part 0000009

Submitted a job which basically run baselines on resampled corpora:

    [minhle@int1 EvEn]$ sbatch scripts/run-steps-on-cartesius1.job
    Submitted batch job 6868870

The queue is long enough to exceed my max screen buffer, with some persons
running a truckload of jobs. Surprisingly, my job often gets a chance!

    [minhle@int1 EvEn]$ squeue | grep VASP544_ | wc -l
    256
    [minhle@int1 EvEn]$ squeue | grep T4L_EM | wc -l
    75
    [minhle@int1 EvEn]$ squeue | grep Eq-NICH | wc -l
    62

Fixed some issues, finished adapting baselines to run on resampled data. 
Script is running:

    [minhle@int1 EvEn]$ squeue | grep minh
            6868885    normal run-step   minhle  R    1:08:16      1 tcn880

Being flexible seems quite important. Earlier scripts where the assumption of 
directory structure is hard-coded seems harder to adapt than those
using wildcards to select log files. 
Next: Stanford Sieve, etc.

## Tue 24 Sep 

Bug: systems overwriting each other's logs. Rerunning...

    [minhle@int1 EvEn]$ rm -rf output/baselines
    [minhle@int1 EvEn]$ sbatch scripts/run-steps-on-cartesius1.job
    Submitted batch job 6869198

Baselines are done. The variation is so small it doesn't show up on the figure.

Modified Stanford Sieve scripts to run on samples:

    [minhle@int1 EvEn]$ sbatch scripts/run-steps-on-cartesius1.job
    Submitted batch job 6871589
    [minhle@int1 EvEn]$ tail -f slurm-6871589.out
    ...
    Running on /nfs/home2/minhle/EvEn/output/conll-2012-transformed-resampled/sample_01/no-name/dev_test.m_gold_conll ...
    Running on /nfs/home2/minhle/EvEn/output/conll-2012-transformed-resampled/sample_01/nonmen_100/dev_test.m_gold_conll ...
    Running on /nfs/home2/minhle/EvEn/output/conll-2012-transformed-resampled/sample_01/nonmen_20/dev_test.m_gold_conll ...

## Wed 25 Sep 

Running out of disk space because my scripts are spewing out 20G of CoNLL files at a time.
I wrote a simple rule to clean things up, from now on just need to run 
`drake %save-disk-space`.

Restarted calculating Stanford Sieve results:

    [minhle@int1 EvEn]$ rm -rf output/stanford-sieve/
    [minhle@int1 EvEn]$ sbatch scripts/run-steps-on-cartesius1.job
    Submitted batch job 6872290

I used 10% of my budget so far. Since everything is increased 25-fold now, not sure
if I can finish e2e evaluation in budget...

Continue with `cort`... My 80/20 rule is veering toward 99% dump work and 1% interesting one.

## Thu 26 Sep

Sieve results arrived and small error bars can now be seen on the plot. Yay!

Running cort now:

    [minhle@int1 EvEn]$ rm -rf output/cort/eval
    [minhle@int1 EvEn]$ sbatch scripts/run-steps-on-cartesius1.job
    Submitted batch job 6876736

Running out of disk space again. The bottleneck is at the CoNLL-2012 corpus exported
for deep-coref. There are many versions of them which take up 60G of the 200G quota I have.
I'll need to rewrite my scripts to clean them up after training/evaluation.

    [minhle@int2 EvEn]$ rm -rf output/deep-coref/conll-2012-transformed-exported/

Flexible input paths means smarter way to find the right model to run is needed. 
After fixing, running `cort` again:

    [minhle@int1 EvEn]$ sbatch scripts/run-steps-on-cartesius1.job
    Submitted batch job 6876813

## Fri 27 Sep

Got new `cort` results. It's striking how much trouble we need to go through to get
a tiny whisker on top of the bars.

## Tue 1 Oct

Working on minimizing resampled corpora into e2e format while I accidentally deleted 
all resample corpora >"< This would lead to the rerunning of many steps unless I 
[reset the modification time to some point in the past](https://www.thegeekstuff.com/2012/11/linux-touch-command/).

Need to wait a few hours again because of this stupid mistake >"<

## Fri 4 Oct

Used this command to avoid rerunning steps that are dependent on resampled corpora:

    find output/conll-2012-transformed-resampled -exec touch -m -t 07281200 {} \;

Generated some scripts to evaluate e2e models. Trying one of them:

    [minhle@int2 EvEn]$ sbatch output/e2e/eval_scripts/04.job
    Submitted batch job 6907736
    [minhle@int2 EvEn]$ squeue | grep minh
            6907736    normal   04.job   minhle PD       0:00      1 (Priority)

## Mon 7 Oct

e2e evaluation ran but broke down because of a difference in the document part ID at the
beginning of the document and the second column. Using the same part would lead to documents
having the same ID which will mess with the official scorer. So I need to change the second
column accordingly... Tested on two versions of CoNLL, running on all:

    [minhle@int2 EvEn]$ sbatch scripts/run-steps-on-cartesius1.job
    Submitted batch job 6932464

Trying to rerun e2e:

    [minhle@int2 EvEn]$ sbatch output/e2e/eval_scripts/04.job
    Submitted batch job 6933170

Submitted a bunch of jobs:

    [minhle@int1 EvEn]$ scripts/run-steps-on-cartesius2b.sh
    --- 19. Running (timestamped): /nfs/home2/minhle/EvEn/././output/e2e/eval <- /nfs/home2/minhle/EvEn/././output/e2e/eval_scripts
    Submitted batch job 6934991
    Submitted batch job 6934992
    Submitted batch job 6934993
    Submitted batch job 6934994
    Submitted batch job 6934995
    Submitted batch job 6934996
    Submitted batch job 6934997
    Submitted batch job 6934998
    Submitted batch job 6934999
    Submitted batch job 6935001
    Submitted batch job 6935002
    Submitted batch job 6935003
    Submitted batch job 6935004
    Submitted batch job 6935005
    Submitted batch job 6935006
    Submitted batch job 6935007

Finished evaluating e2e. Now preparing data for deep-coref:

    [minhle@int1 EvEn]$ sbatch scripts/run-steps-on-cartesius1.job
    Submitted batch job 6935648

Turned out the last time I deleted exported corpora, the trained models were gone with them.
To deal with disk quota, I'll have to use the scratch space, which only holds a file for
a maximum of 14 days.

# Mon 14 Oct

meeting with Antske

- pair bootstrapping —> what about it?? looked up but still don’t understand
- mid nov finish, get Piek+Antske’s feedback
- interpreting name experiments: dianogstic classification (whether net retains information about seen entities) + visualization

# Mon 21 Oct

Somehow I lost the results of the CoNLL corpora resampled for Deep-coref...
Couldn't find out why yet...? Luckily that I have recorded everything in a Drakefile.

# Sun 3 Nov

TODO:

- get error bars for human annotations
- update paper
- check what still needs to be done

I might have figured out why deep-coref performs so poorly on mention-masked docs.
Added random-shuffled baseline.

# Thu 7 Nov

Evaluating random-shuffled baseline.

Revising paper, stopped at section 4.2

# Fri 8 Nov

Due to a bug, the shuffling was ineffectual. Fixed and rerunning...

    commit 100ce4de47a09921156d99f8dad09a76346e6764
    [minhle@int2 EvEn]$ sbatch scripts/run-steps-on-cartesius1.job
    Submitted batch job 7112117

Resampled human annotation results are done, updated figures

# Sat 9 Nov

It turns out that random-antecedent with shuffling achieves virtually the same
performance to one without shuffling...

Second hypothesis: maybe deep-coref performs poorly because it do agglomeration
instead of mention ranking. The former creates a lot more clusters and
doesn't favor bigger clusters.

    commit a848a7b8494ac08533ff1f23ac6be9b2e659839b
    [minhle@int2 EvEn]$ sbatch scripts/run-steps-on-cartesius1.job
    Submitted batch job 7114818

Nope, it's not... Random agglomeration performs a bit more poorly but still get 33%,
not the ~5% that deep-coref gets on men-100.

# Mon 25 Nov

Meeting with Antske and Piek:

- my modified deep-coref seems to work actually
- check why it performs so poorly in the "- mention" settings
    - look at output?
    - look at embeddings, enable/disable them??

# Tue 26 Nov

Generated scoring form, slowly working on it: 
`data/annotations/entity-guessing-scored.ods`

# Wed 27 Nov

Finished evaluating entity guessing.

# Sat 7 Dec 2019

Came up with a plan to get resampling results for deep-coref:
symbolic-link model folders under resampled corpus.

Started training deep-coref models, put in scratch to avoid hitting up against 
disk quota. The only problem is that the models will be wiped out after 2 weeks.

Working on "related work" section...

# Mon 9 Dec 2019

Shit hits the fan, again... Some models haven't finished training yet 
(`reward_rescaling` is missing).

    /scratch-shared/minhle/EvEn/output/deep-coref/conll-2012-transformed-exported/no-external/models:
    all_pairs  top_pairs

    /scratch-shared/minhle/EvEn/output/deep-coref/conll-2012-transformed-exported/no-match/models:
    all_pairs  top_pairs

    /scratch-shared/minhle/EvEn/output/deep-coref/conll-2012-transformed-exported/no-name/models:
    all_pairs  top_pairs

    /scratch-shared/minhle/EvEn/output/deep-coref/conll-2012-transformed-exported/nonmen_20/models:
    all_pairs  top_pairs

    /scratch-shared/minhle/EvEn/output/deep-coref/conll-2012-transformed-exported/orig/models:
    all_pairs  top_pairs

The missing models distribute randomly so I think it's a performance issue instead of some bugs.
I reran training with 2 models per node instead of 4. Hopefully, the script will pick up where
it left off instead of starting all over again.

    [minhle@int2 EvEn]$ !sque
    squeue | grep minh
           7321384       gpu train-mu   minhle PD       0:00      1 (Priority)
           7321385       gpu train-mu   minhle PD       0:00      1 (Priority)
           7321386       gpu train-mu   minhle PD       0:00      1 (Priority)
           7321387       gpu train-mu   minhle PD       0:00      1 (Priority)
           7321388       gpu train-mu   minhle PD       0:00      1 (Priority)
           7321389       gpu train-mu   minhle PD       0:00      1 (Priority)
           7321390       gpu train-mu   minhle PD       0:00      1 (Priority)
           7321391       gpu train-mu   minhle PD       0:00      1 (Priority)

The problem is, because I ran training on 4 GPU nodes for 2 days already, my priority 
seems to have gone down...

Noticed some weird behavior of deep-coref: the `CN` (correct-new?) figure is much
lower for men100 which might explain the low scores. It's weird that the total
(CN+CL+FN+FL+WL) is different for each model. Could it be that some mentions went 
missing?

    [minhle@int2 EvEn]$ tail output/deep-coref/logs/orig.log
    test - loss: 0.5515 - CN: 20284 - CL: 53197 - FN: 4260 - FL: 2741 - WL: 2153
    [minhle@int2 EvEn]$ tail output/deep-coref/logs/nonmen_100.log
    test - loss: 0.5748 - CN: 19735 - CL: 23737 - FN: 1519 - FL: 1734 - WL: 1922
    [minhle@int2 EvEn]$ tail output/deep-coref/logs/men_100.log
    test - loss: 0.1553 - CN: 225 - CL: 41238 - FN: 20 - FL: 2922 - WL: 404

# Mon 16 Dec

It really showed up in the file sizes that a lot of mentions are missing:

    [minhle@int2 EvEn]$ scratch=/scratch-shared/minhle/EvEn
    [minhle@int2 EvEn]$ du -sh $scratch/output/deep-coref/conll-2012-transformed-exported/*
    2.2G	/scratch-shared/minhle/EvEn/output/deep-coref/conll-2012-transformed-exported/men_100
    4.3G	/scratch-shared/minhle/EvEn/output/deep-coref/conll-2012-transformed-exported/men_20
    3.8G	/scratch-shared/minhle/EvEn/output/deep-coref/conll-2012-transformed-exported/men_40
    3.2G	/scratch-shared/minhle/EvEn/output/deep-coref/conll-2012-transformed-exported/men_60
    2.6G	/scratch-shared/minhle/EvEn/output/deep-coref/conll-2012-transformed-exported/men_80
    4.8G	/scratch-shared/minhle/EvEn/output/deep-coref/conll-2012-transformed-exported/no-external
    4.8G	/scratch-shared/minhle/EvEn/output/deep-coref/conll-2012-transformed-exported/no-internal
    4.8G	/scratch-shared/minhle/EvEn/output/deep-coref/conll-2012-transformed-exported/no-match
    4.8G	/scratch-shared/minhle/EvEn/output/deep-coref/conll-2012-transformed-exported/no-name
    2.4G	/scratch-shared/minhle/EvEn/output/deep-coref/conll-2012-transformed-exported/nonmen_100
    4.6G	/scratch-shared/minhle/EvEn/output/deep-coref/conll-2012-transformed-exported/nonmen_20
    4.3G	/scratch-shared/minhle/EvEn/output/deep-coref/conll-2012-transformed-exported/nonmen_40
    3.9G	/scratch-shared/minhle/EvEn/output/deep-coref/conll-2012-transformed-exported/nonmen_60
    3.3G	/scratch-shared/minhle/EvEn/output/deep-coref/conll-2012-transformed-exported/nonmen_80
    4.8G	/scratch-shared/minhle/EvEn/output/deep-coref/conll-2012-transformed-exported/orig

I seem to have found the problem: I need to set `-coref.md.useGoldMentions true`
otherwise, deep-coref won't use the provided mentions. Fixed and started
exporting again:

    [minhle@int2 EvEn]$ sbatch scripts/run-steps-on-cartesius1.job
    Submitted batch job 7350808
    [minhle@int2 EvEn]$ !sque
    squeue | grep minh
            7350808    normal run-step   minhle  R       0:10      1 tcn864
    [minhle@int2 EvEn]$ tail -f slurm-7350808.out

Finished exporting training sets, started training while exporting resampled
corpora for evaluation.

    [minhle@int1 EvEn]$ !sque
    squeue | grep minh
            7351581       gpu train-mu   minhle  R       0:03      1 gcn28
            7351582       gpu train-mu   minhle  R       0:03      1 gcn30
            7351575       gpu train-mu   minhle  R       0:05      1 gcn17
            7351576       gpu train-mu   minhle  R       0:05      1 gcn18
            7351577       gpu train-mu   minhle  R       0:05      1 gcn57
            7351578       gpu train-mu   minhle  R       0:05      1 gcn61
            7351579       gpu train-mu   minhle  R       0:05      1 gcn62
            7351580       gpu train-mu   minhle  R       0:05      1 gcn63
            7351556    normal run-step   minhle  R       4:41      1 tcn872

I expected `drake` to run all necessary steps but it instead only runs one
step at a time, forcing me to repeat this command again and again:

    [minhle@int1 EvEn]$ sbatch scripts/run-steps-on-cartesius1.job
    Submitted batch job 7353820

It's working:

    [minhle@int1 EvEn]$ ls -lh /scratch-shared/minhle/EvEn/output/deep-coref/conll-2012-transformed-resampled-exported/*/*/gold/dev_test
    -rw-r--r-- 1 minhle minhle 13K Dec 16 16:24 /scratch-shared/minhle/EvEn/output/deep-coref/conll-2012-transformed-resampled-exported/sample_00/men_100/gold/dev_test
    -rw-r--r-- 1 minhle minhle 42K Dec 16 16:24 /scratch-shared/minhle/EvEn/output/deep-coref/conll-2012-transformed-resampled-exported/sample_00/men_20/gold/dev_test
    -rw-r--r-- 1 minhle minhle 32K Dec 16 16:24 /scratch-shared/minhle/EvEn/output/deep-coref/conll-2012-transformed-resampled-exported/sample_00/men_40/gold/dev_test
    -rw-r--r-- 1 minhle minhle 25K Dec 16 16:24 /scratch-shared/minhle/EvEn/output/deep-coref/conll-2012-transformed-resampled-exported/sample_00/men_60/gold/dev_test
    -rw-r--r-- 1 minhle minhle 16K Dec 16 16:24 /scratch-shared/minhle/EvEn/output/deep-coref/conll-2012-transformed-resampled-exported/sample_00/men_80/gold/dev_test
    -rw-r--r-- 1 minhle minhle 50K Dec 16 16:24 /scratch-shared/minhle/EvEn/output/deep-coref/conll-2012-transformed-resampled-exported/sample_00/no-external/gold/dev_test

Paper is almost complete... 

# Thu 19 Dec

Ran deep-coref evaluation but there was no variance because the code fails
to use my resampled corpora. Plan: run export and train new models directly
on resampled corpora. I still have 3/4 of my budget, 
if I train for 1 day only, perhaps it will be enough... Any other way?

Ran this to make sure that I won't lose files for the next 2 weeks:

    [minhle@int2 EvEn]$ cd ~/scratch/
    [minhle@int2 scratch]$ find . -exec touch {} \;

# Thu 26 Dec

Ran the serial touch command again in `screen` to avoid losing files.

# Mon 10 Feb 2020

The TACL submission was rejected. Now, we need to run experiments on
another dataset and a BERT model as well...

# Mon 17 Feb 2020

Had a meeting with Piek and Antske, decided to go for COLING.

TODO: fix deep-coref evaluation