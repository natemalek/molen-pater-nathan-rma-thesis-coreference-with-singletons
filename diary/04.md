## Fri 16 Feb 2018

Pick up this project again. I will write a very good paper with it!

## Fri 6 Apr

Last weeks I prepared a draft for a meeting with Piek and Antske.

I simplified the transformations and came up with a new idea: masking some
tokens (1%, 20%, etc.) to see if systems and humans can fill in the gap.
Antske seems to think it's a good idea.

I spent Friday on finishing the draft and implement my ideas. I want to have 
a quick look to see if the models behave like what I expect. 

## Sun 8 Apr

Fixed some problems with the code. All but one test cases are running.
I will start a job on DAS-5 to generate the transformed corpora. It takes 
surprisingly long time.

## Mon 9 Apr

Job terminated with an error after running for 8 hours. Damn!

    Reading data/conll-2012-flat/train/pt_nt_4212.v4_gold_conll... Done.
    PennTreeReader: warning: incomplete tree (extra left parentheses in input): (TOP (XX <MASKED>) (IN <MASKED>) (RB <MASKED>) (RB <MASKED>) (, <MASKED>) (DT <MASKED>) (NN <MASKED>) (IN <MASKED>) (CD <MASKED>) (MD <MASKED>) (VB <MASKED>) (VBN <MASKED>) (, <MASKED>) (CD <MASKED>) (IN <MASKED>) (CD <MASKED>) (, <MASKED>) (CC <MASKED>) (CD <MASKED>) (IN <MASKED>) (CD <MASKED>) (. <MASKED>))
    Traceback (most recent call last):
      File "manipulate_corpus2.py", line 40, in <module>
        manipulate_func('data/conll-2012-flat/train', paths.train_path)
      File "/var/scratch/minhle/EvEn/manipulations.py", line 169, in __call__
        new_doc = self.apply_doc(doc)
      File "/var/scratch/minhle/EvEn/manipulations2.py", line 118, in apply_doc
        return reparse(doc)
      File "/var/scratch/minhle/EvEn/manipulations.py", line 478, in reparse
        for row in new_doc_table))
      File "/home/minhle/.local/lib/python3.5/site-packages/cort-0.2.4.5-py3.5.egg/cort/core/documents.py", line 402, in __init__
        include_erased=True # see https://github.com/smartschat/cort/issues/13
      File "/home/minhle/.local/lib/python3.5/site-packages/StanfordDependencies/StanfordDependencies.py", line 116, in convert_trees
        for ptb_tree in ptb_trees)
      File "/home/minhle/.local/lib/python3.5/site-packages/StanfordDependencies/StanfordDependencies.py", line 116, in <genexpr>
        for ptb_tree in ptb_trees)
      File "/home/minhle/.local/lib/python3.5/site-packages/StanfordDependencies/JPypeBackend.py", line 89, in convert_tree
        raise ValueError("Invalid Penn Treebank tree: %r" % ptb_tree)
    ValueError: Invalid Penn Treebank tree: '(TOP (XX <MASKED>) (IN <MASKED>) (RB <MASKED>) (RB <MASKED>) (, <MASKED>) (DT <MASKED>) (NN <MASKED>) (IN <MASKED>) (CD <MASKED>) (MD <MASKED>) (VB <MASKED>) (VBN <MASKED>) (, <MASKED>) (CD <MASKED>) (IN <MASKED>) (CD <MASKED>) (, <MASKED>) (CC <MASKED>) (CD <MASKED>) (IN <MASKED>) (CD <MASKED>) (. <MASKED>)'

## Thu 12 Apr

Make the program work for `data/conll-2012-flat/train/pt_nt_4212.v4_gold_conll`.
Though I don't really understand why.
Found out that I was copying the syntax column to all semantic columns. Fixed. 
Hope that it works correctly for everything now.

Restarted the program:

    $ sbatch manipulate_corpus2.job
    Submitted batch job 1791457

Forgot to remove a debugging line. It created a encoding error.

## Fri 13 Apr

Turns out I didn't fix the "incomplete tree" bug yesterday. Found out why.
Rerun everything. 

Finished after 9 hour.

## Sun 15 Apr

Tried to run some scripts:

    [minhle@fs0 EvEn]$ scripts/run-sieve-auto.sh /var/scratch/minhle/EvEn/output/conll-2012-manipulated2.2018-04-13-271bf1e/men_100/dev
    ...
    Apr 15, 2018 12:12:40 PM edu.stanford.nlp.dcoref.SieveCoreferenceSystem printFinalConllScore
    INFO: Final conll score ((muc+bcub+ceafe)/3) = 39.12
    Apr 15, 2018 12:12:40 PM edu.stanford.nlp.dcoref.SieveCoreferenceSystem getFinalScore
    INFO: Final score (pairwise) Precision = 0.11
    Apr 15, 2018 12:12:40 PM edu.stanford.nlp.dcoref.SieveCoreferenceSystem initializeAndRunCoref
    INFO: done
        

    [minhle@fs0 EvEn]$ scripts/run-deep-coref-gold.sh /var/scratch/minhle/EvEn/data/conll-2012-flat/dev  
    METRIC bcub:Coreference: Recall: (13174.76 / 19134) 68.85%  Precision: (13611.79 / 15753) 86.4% F1: 76.63%
    METRIC ceafm:Coreference: Recall: (13493 / 19134) 70.51%    Precision: (13493 / 15753) 85.65%   F1: 77.35%
    METRIC ceafe:Coreference: Recall: (2852.7 / 4537) 62.87%    Precision: (2852.7 / 3416) 83.5%    F1: 71.73%
    METRIC blanc:Coreference links: Recall: (73581 / 98774) 74.49%  Precision: (73581 / 91935) 80.03%   F1: 77.16%
    Non-coreference links: Recall: (538364 / 822311) 65.46% Precision: (538364 / 553075) 97.34% F1: 78.28%
    BLANC: Recall: (0.7 / 1) 69.98% Precision: (0.89 / 1) 88.68%    F1: 77.72%
    [main] INFO CoreNLP - Final conll score ((muc+bcub+ceafe)/3) = 77.83
            
            
    [minhle@node053 EvEn]$ scripts/run-deep-coref-gold.sh /var/scratch/minhle/EvEn/output/conll-2012-manipulated2.2018-04-13-271bf1e/nonmen_50/dev
    METRIC bcub:Coreference: Recall: (11114.05 / 18577) 59.82%  Precision: (11945.94 / 14277) 83.67%    F1: 69.76%
    METRIC ceafm:Coreference: Recall: (11696 / 18577) 62.95%    Precision: (11696 / 14277) 81.92%   F1: 71.19%
    METRIC ceafe:Coreference: Recall: (2435.93 / 4503) 54.09%   Precision: (2435.93 / 3081) 79.06%  F1: 64.23%
    METRIC blanc:Coreference links: Recall: (58616 / 95041) 61.67%  Precision: (58616 / 78937) 74.25%   F1: 67.38%
    Non-coreference links: Recall: (452680 / 792411) 57.12% Precision: (452680 / 475095) 95.28% F1: 71.42%
    BLANC: Recall: (0.59 / 1) 59.4% Precision: (0.85 / 1) 84.76%    F1: 69.4%
    [main] INFO CoreNLP - Final conll score ((muc+bcub+ceafe)/3) = 71.67
    
    
    [minhle@node053 EvEn]$ scripts/run-deep-coref-gold.sh /var/scratch/minhle/EvEn/output/conll-2012-manipulated2.2018-04-13-271bf1e/nonmen_100/dev            
    ...
    METRIC bcub:Coreference: Recall: (7761.3 / 18589) 41.75%    Precision: (9242.98 / 11478) 80.52% F1: 54.99%
    METRIC ceafm:Coreference: Recall: (8675 / 18589) 46.66% Precision: (8675 / 11478) 75.57%    F1: 57.7%
    METRIC ceafe:Coreference: Recall: (1737.28 / 4509) 38.52%   Precision: (1737.28 / 2536) 68.5%   F1: 49.31%
    METRIC blanc:Coreference links: Recall: (41147 / 95048) 43.29%  Precision: (41147 / 61083) 67.36%   F1: 52.7%
    Non-coreference links: Recall: (294752 / 792518) 37.19% Precision: (294752 / 317012) 92.97% F1: 53.13%
    BLANC: Recall: (0.4 / 1) 40.24% Precision: (0.8 / 1) 80.17% F1: 52.91%
    [main] INFO CoreNLP - Final conll score ((muc+bcub+ceafe)/3) = 57.89

Stanford Sieve and Stanford neural net models seem to run. I'll try running 
them first.
    
    [minhle@fs0 EvEn]$ python3 version.py
    2018-04-15-446e97d
    [minhle@fs0 EvEn]$ sbatch exp_coref_matrix.job
    Submitted batch job 1792707

## Mon 16 Apr

Downloaded all output: `scp -P 8098 -r minhle@127.0.0.1:/var/scratch/minhle/EvEn/output/exp_coref_matrix-2018-04-15-446e97d /Users/cumeo/Projects/spinoza/ulm-4/EvEn/output/`  

## Fri 25 May 2018

I decided not to use AllenNLP's package because I couldn't find out how to 
run it on OntoNotes/CoNLL 2012.

Tried to run e2e, it requires Python 2.7 while mine is Python 3. I decided
that I will run all systems on Docker images instead.

Antske suggested using Singularity instead of Docker because it doesn't 
require root privileges.

## Fri 1 Jun 2018

Made some progress with preparing a docker image for e2e. It takes a lot of 
time because you need to do everything twice or more but I think it's worth it.

Docker is not available on Cartesius so I will try to use Singularity indeed.
Got "libarchive not found", wrote to Cartesius. Looks like Docker needs special
configuration to enable GPU (?) and similar to Singularity (?). 
Reading [this](https://userinfo.surfsara.nl/systems/shared/software/Singularity), 
I can't find out how to use Singularity on Cartesius without root privilege.

Ran into another problem with memory limit in Docker. Although I had tried
`-m 16g`, the memory usage still doesn't raise about 2g and the demo program
is stills killed. 

I'm inclined NOT to use Docker or Singularity because they're too complicated
and risky. I wrote to Cartesius people anyway albeit without much hope.

Preparing the datasets is too long so I decided to submit a job. With `nohup` 
and `srun`, it's quite convenient: 

    [minhle@int1 e2e-coref]$ nohup srun -t  2-00:00:00 ./setup_training.sh & 
    [minhle@int1 e2e-coref]$ squeue | grep minh
               4301995    normal setup_tr   minhle  R       7:08      1 tcn526

Submitted another job to get test results:

    [minhle@int1 e2e-coref]$ nohup srun -d 4301995 -t 10:00:00 python test_single.py best > ../output/e2e/test-single-best.out 2>&1 &
    [minhle@int1 e2e-coref]$ !squeu
    squeue | grep minh
               4302063    normal   python   minhle PD       0:00      1 (Dependency)
               4301995    normal setup_tr   minhle  R      12:48      1 tcn526

## Mon 4 Jun

srun commands were terminated and I don't really know why. Wrote a sbatch
script instead:

    [minhle@int1 EvEn]$ sbatch scripts/e2e/test-single.job
    Submitted batch job 4313073

## Fri 15 Jun

Fixed some problems with the custom GPU kernel. I needed to downgrade tensorflow
to 1.3.0 for compatibility with CUDA 8/CUDNN 6 we have on Cartesius.

Training one model:

    [minhle@int2 EvEn]$ sbatch scripts/e2e/train-single.job
    Submitted batch job 4333285

I can't evaluate their "best" model because they don't share it but I can 
evaluate their "final" model. I checked out the code on my laptop to ease
development.

## Sun 17 Jun

Preparing to evaluate e2e on Cartesius: 

    [minhle@int1 EvEn]$ nohup output/minimize_datasets.2018-06-16-b3f5d9b.sh 2>&1 > output/minimize_datasets.2018-06-16-b3f5d9b.sh.out &
    [1] 9582

I almost finished one evaluation of e2e but got a strange error. Reruning
`setup_training.sh` to make sure that everything is prepared correctly.

    [minhle@int2 EvEn]$ . scripts/e2e/cd.sh
    (venv) [minhle@int2 e2e-coref]$ nohup ./setup_training.sh > ../output/e2e/setup_training.sh.out 2>&1 &
    [1] 1110
    ...
    Wrote 343 documents to dev.english.jsonlines
    Minimizing train.english.v4_auto_conll
    Merging clusters (shouldn't happen very often.)
    Wrote 2802 documents to train.english.jsonlines
    Minimizing test.english.v4_gold_conll
    Wrote 348 documents to test.english.jsonlines
    Wrote 114 characters to char_vocab.english.txt
    Found 47311 words in 3 dataset(s).
    Kept 43994 out of 2196017 lines.
    Wrote result to glove.840B.300d.txt.filtered.

## Thu 21 Jun

Started one sample evaluation:

    (venv) [minhle@int2 EvEn]$ nohup scripts/e2e/test-single.job > output/e2e/test-single.job.out 2>&1 &
    [1] 20267
    (venv) [minhle@int2 EvEn]$ tail -f output/e2e/test-single.job.out
    ...
    ====== TOTALS =======
    Identification of Mentions: Recall: (10890 / 19764) 55.1%   Precision: (10890 / 12024) 90.56%   F1: 68.51%
    --------------------------------------------------------------------------
    Coreference: Recall: (1481.12663749861 / 4532) 32.68%   Precision: (1481.12663749861 / 2556) 57.94% F1: 41.79%
    --------------------------------------------------------------------------
    
    Average F1 (conll): 49.02%
    Average F1 (py): 49.03%
    Average precision (py): 68.03%
    Average recall (py): 38.43%

Started all experiments at once:

    [minhle@int1 EvEn]$ . scripts/setup-cartesius.sh
    [minhle@int1 EvEn]$ nohup output/e2e/evaluate_all.2018-06-21-a8cf20b.sh > output/e2e/evaluate_all.2018-06-21-a8cf20b.sh.out 2>&1 &
    [1] 16964
    [minhle@int1 EvEn]$ tail -f output/e2e/evaluate_all.2018-06-21-a8cf20b.sh.out
    ...
    Loaded 342 eval examples.
    Evaluated 1/342 examples.
    Evaluated 11/342 examples.
    Evaluated 21/342 examples.
    Evaluated 31/342 examples.
    Evaluated 41/342 examples.

## Fri 22 Jun 2018

Experiments failed because model folders are missing. I'll create symlinks
to their final model.

Restarting evaluation:

    [minhle@int1 EvEn]$ python3 version.py
    2018-06-22-caf9c0d
    [minhle@int1 EvEn]$ python3 scripts/e2e/gen_experiments_conf.py
    Wrote 28 configurations to e2e-coref/experiments.conf
    Wrote commands to output/e2e/evaluate_all.2018-06-22-caf9c0d.sh
    [minhle@int1 EvEn]$ . scripts/setup-cartesius.sh
    [minhle@int1 EvEn]$ nohup output/e2e/evaluate_all.2018-06-22-caf9c0d.sh > output/e2e/evaluate_all.2018-06-22-caf9c0d.sh.out 2>&1 &
    [1] 10971
    [minhle@int1 EvEn]$ tail -f output/e2e/evaluate_all.2018-06-22-caf9c0d.sh.out
    ...
    Evaluating logs/final-inst-dev/model.max.ckpt
    OOV rate for glove.840B.300d.txt: 1.50%
    OOV rate for turian.50d.txt: 2.00%
    Loaded 342 eval examples.
    Evaluated 1/342 examples.
    Evaluated 11/342 examples.

## Fri 13 Jul 2018

Realized two things:

1. Cartesius automatically remove files in scratch folder that are older than 
14 days, that's why my files keep vanishing --> move files back to home folder.
2. My latest results weren't using gold mention spans --> try to modify e2e
to take into account gold spans instead of computing by itself

## Sun 15 July

Tried to test using gold spans but wasn't successful:

    [minhle@int1 e2e-coref]$ srun -p gpu_short -t 1:00:00 --pty bash
    bash: unalias: python: not found
    [minhle@gcn5 e2e-coref]$ . ../scripts/setup-cartesius.sh
    [minhle@gcn5 e2e-coref]$ . ../output/e2e/venv/bin/activate
    (venv) [minhle@gcn5 e2e-coref]$ ln -s `pwd`/logs/final logs/final-nonmen_75-dev
    (venv) [minhle@gcn5 e2e-coref]$ python -u test_single.py final-nonmen_75-dev

It turns out the directory on Cartesius is full of holes because they 
automatically remove files that are older than 14 days (thanks, Cartesius!)
I need to rerun a whole bunch of steps, starting from here:

    [minhle@int2 EvEn]$ git checkout 271bf1e
    HEAD is now at 271bf1e... forgot to remove debug code again
    [minhle@int2 EvEn]$ nohup ./manipulate_corpus2.job > ./manipulate_corpus2.job.out 2>&1 &

## Fri 20 July 2018

Running e2e-coref on Cartesius, the last time, the "Identification of Mentions"
metrics is low (~50%). <s>I'm expecting it to be 100% this time because I 
programmed it to use gold spans.</s>

    [minhle@int2 EvEn]$ . scripts/setup-cartesius.sh
    [minhle@int2 EvEn]$ . output/e2e/venv/bin/activate
    (venv) [minhle@int2 EvEn]$ cd e2e-coref
    (venv) [minhle@int2 e2e-coref]$ nohup python -u test_single.py final > ../try-gold-spans.out 2>&1 &
    [1] 10942

No, that doesn't work. I need to retrain the model instead because the 
computational graph is fixed after training. 

    [minhle@int1 EvEn]$ sbatch scripts/e2e/train-single.job
    Submitted batch job 4494428

Now it's training but it doesn't stop. I need to send sigint (Ctrl-C) manually
which is not possible using slurm. The only option is to cancel the whole job.
Updated the job bash file to to include evaluator.py. Restarting the job...

    [minhle@int1 EvEn]$ sbatch scripts/e2e/train-single.job
    Submitted batch job 4494724

The design prevents this experiment from being reproduced exactly: TensorFlow
removes models based on wall-clock time, the evaluator runs in parallel and 
would pick up any model that comes its way.

    ====== TOTALS =======
    Identification of Mentions: Recall: (13774 / 19155) 71.9%   Precision: (13774 / 16015) 86%  F1: 78.32%
    --------------------------------------------------------------------------
    Coreference: Recall: (2392.93693623975 / 4546) 52.63%   Precision: (2392.93693623975 / 3864) 61.92% F1: 56.9%
    --------------------------------------------------------------------------

The results are still not good -- we're still missing some mentions. I wonder why...
Yeah, I replaced candidate mentions with gold mentions but forgot to replace
the predicted mentions too. Let's try again...

    [minhle@int1 EvEn]$ python3 version.py
    2018-07-20-9683407
    [minhle@int1 EvEn]$ sbatch scripts/e2e/train-single.job
    Submitted batch job 4495030

Cartesius is rather busy now... I'll check the job later.

## Wed 25 Jul

Nope, it still doesn't get 100% in identification:

    [minhle@int1 EvEn]$ . scripts/setup-cartesius.sh
    [minhle@int1 EvEn]$ . output/e2e/venv/bin/activate
    (venv) [minhle@int1 EvEn]$ cd e2e-coref
    (venv) [minhle@int1 e2e-coref]$ python -u test_single.py best
    ...
    ====== TOTALS =======
    Identification of Mentions: Recall: (14995 / 19155) 78.28%  Precision: (14995 / 17746) 84.49%   F1: 81.27%
    --------------------------------------------------------------------------
    Coreference: Recall: (2704.54796974034 / 4546) 59.49%   Precision: (2704.54796974034 / 4223) 64.04% F1: 61.68%
    --------------------------------------------------------------------------
    
## Fri 27 Jul

Found a typo. It turns umout that I didn't use gold mention spans at all.

I'm going to restart training. Below are some evaluation results of the first
training iterations from last time. I expect the new numbers to be higher.

    [minhle@int2 EvEn]$ grep "Average F1 (conll):" slurm4495030-evaluator.out  | head
    Average F1 (conll): 52.91%
    Average F1 (conll): 52.89%
    Average F1 (conll): 53.87%
    Average F1 (conll): 57.28%
    Average F1 (conll): 55.35%
    Average F1 (conll): 57.04%
    Average F1 (conll): 57.49%
    Average F1 (conll): 57.87%
    Average F1 (conll): 57.63%
    Average F1 (conll): 59.60%

Got a problem with maximum span size. I measured the empirical maximum size
by adding some code to coref_model.py:64:

Start training again, waiting for my turn...

    [minhle@int1 EvEn]$ sbatch scripts/e2e/train-single.job
    Submitted batch job 4515049
    [minhle@int1 EvEn]$ squeue | grep minh
               4515049       gpu train-si   minhle PD       0:00      1 (Priority)

In the meantime, I'll work on running cort.

Trying to run cort in debugger to trace its steps. I keep getting
error when running it on my Mac whereas it runs on Cartesisu without any problem.

    2018-07-28 00:15:23,350 INFO Loading model.
    /Users/cumeo/.pyxbld/temp.macosx-10.12-x86_64-3.6/pyrex/cort/coreference/perceptrons.c:567:10: fatal error:
        'numpy/arrayobject.h' file not found
    #include "numpy/arrayobject.h"
            ^~~~~~~~~~~~~~~~~~~~~
    1 error generated.
    ...
    ImportError: Building module cort.coreference.perceptrons failed: ["distutils.errors.CompileError: command 'clang' failed with exit status 1\n"]

**UPDATE (Sat 8 Sep 2018): fixed by running this command**

    Minhs-MacBook-Pro:EvEn cumeo$ export CFLAGS=-I`python3 -c 'import numpy; print(numpy.get_include())'`

For whatever reason, my manipulated corpora are all empty... They are not removed
but just contain nothing. I'll rerun the generation and consolidation of corpora.

    [minhle@int1 EvEn]$ cat output/prepare-corpora.sh
    git reset --hard && \
        git checkout 271bf1e && ./manipulate_corpus2.job && \
        git checkout 3530f06 && python3 consolidate_copora.py
    [minhle@int1 EvEn]$ nohup output/prepare-corpora.sh &

For whatever reason, when I use pipenv instead of system python or 
low-level virtualenv python, I could run `cort-predict-conll` in debug mode
without any problem. Anyway, now I know that the function that takes care 
of extracting mentions is called `__extract_mention_spans_for_sentence`.
I'll work on adapting it tomorrow.

## Sat 28 July 2018

I still haven't recovered from Cartesius's brutal expiration policy :-(
-- keep noticing missing files and empty folders. I'll rerun data preprocessing 
again from the beginning.

    [minhle@int1 EvEn]$ nohup scripts/setup-conll-2012.sh > output/setup-conll-2012.sh.out 2>&1 &
    [1] 3417

    [minhle@int2 EvEn]$ git checkout 2a8f166
    Previous HEAD position was 9683407... adopt conll-2012 from e2e, use gold mention spans for e2e
    HEAD is now at 2a8f166... fixing things, about to rebuild data
    [minhle@int2 EvEn]$ nohup python3 create-flattened-conll-2012.py > output/create-flattened-conll-2012.py.out 2>&1 &
    [1] 10635

    [minhle@int2 EvEn]$ git checkout 7c4e06d
    Previous HEAD position was 952f185... use cartesius to run job
    HEAD is now at 7c4e06d... fix delete_subtree calls
    [minhle@int2 EvEn]$ sbatch ./manipulate_corpus2.job
    Submitted batch job 4525492
    [minhle@int2 EvEn]$ tail -f slurm-4525492.out
    Started: Sun Jul 29 01:06:09 CEST 2018

## Sun 29 Jul

Somehow I got errors again...

    Reading data/conll-2012-flat/train/mz_ectb_1005.v4_auto_conll... doc length: 37983
    doc length: 78487
    doc length: 49585
    doc length: 79635
    doc length: 42235
    doc length: 46823
    Done.
    doc length: 17410
    doc length: 34437
    doc length: 23181
    Traceback (most recent call last):
    File "manipulate_corpus2.py", line 41, in <module>
        manipulate_func('data/conll-2012-flat/train', paths.train_path)
    File "/nfs/home2/minhle/EvEn/manipulations.py", line 169, in __call__
        new_doc = self.apply_doc(doc)
    File "/nfs/home2/minhle/EvEn/manipulations2.py", line 103, in apply_doc
        return reparse(doc)
    File "/nfs/home2/minhle/EvEn/manipulations.py", line 467, in reparse
        for row in new_doc_table))
    File "/home/minhle/.local/lib/python3.5/site-packages/cort-0.2.4.5-py3.5.egg/cort/core/documents.py", line 415, in __init__
        super(CoNLLDocument, self).__init__(identifier, sentences, coref)
    File "/home/minhle/.local/lib/python3.5/site-packages/cort-0.2.4.5-py3.5.egg/cort/core/documents.py", line 97, in __init__
        self.annotated_mentions = self.__get_annotated_mentions()
    File "/home/minhle/.local/lib/python3.5/site-packages/cort-0.2.4.5-py3.5.egg/cort/core/documents.py", line 111, in __get_annotated_mentions
        span, self, first_in_gold_entity=set_id not in seen
    File "/home/minhle/.local/lib/python3.5/site-packages/cort-0.2.4.5-py3.5.egg/cort/core/mentions.py", line 174, in from_document
        mention_property_computer.compute_gender(attributes)
    File "/home/minhle/.local/lib/python3.5/site-packages/cort-0.2.4.5-py3.5.egg/cort/core/mention_property_computer.py", line 89, in compute_gender
        if __wordnet_lookup_gender(" ".join(attributes["head"])):
    TypeError: sequence item 0: expected str instance, ParentedTree found

Turns out it's a known bug: https://github.com/smartschat/cort/issues/20
Fixed by creating and using a fork. Run corpora manipulation yet again.

    [minhle@int2 EvEn]$ git checkout e5013bd
    Previous HEAD position was 7c4e06d... fix delete_subtree calls
    HEAD is now at e5013bd... fix known issue #20
    [minhle@int2 EvEn]$ sbatch ./manipulate_corpus2.job
    Submitted batch job 4537291

## Fri 3 Aug 2018

Finished re-processing CoNLL-2012:

    [minhle@int1 EvEn]$ python3 version.py
    2018-08-03-212b944
    [minhle@int1 EvEn]$ python3 consolidate_copora.py
    
Picking up e2e-coref again. I'm grateful that my past self has documented all the steps 
carefully.

    [minhle@int1 EvEn]$ python3 version.py
    2018-08-03-2dc9495
    [minhle@int1 EvEn]$ python3 scripts/e2e/gencmd_minimize_datasets.py
    Wrote commands to output/e2e/minimize_datasets.2018-08-03-2dc9495.sh
    [minhle@int1 EvEn]$ output/e2e/minimize_datasets.2018-08-03-2dc9495.sh
    [minhle@int1 EvEn]$ python3 version.py
    2018-08-03-7e6505c
    [minhle@int1 EvEn]$ python3 scripts/e2e/gen_experiments_conf.py
    Wrote 56 configurations to e2e-coref/experiments.conf
    Wrote 56 commands to output/e2e/evaluate_all.2018-08-03-7e6505c.sh
    [minhle@int1 EvEn]$ nohup output/e2e/evaluate_all.2018-08-03-7e6505c.sh > output/e2e/evaluate_all.2018-08-03-7e6505c.sh.out 2>&1 &

Running evaluation on login node would occupy too much resources for too long.
I switched to running a SLURM job instead.

    [minhle@int1 EvEn]$ python3 version.py
    2018-08-03-ce5991e
    [minhle@int1 EvEn]$ sbatch scripts/e2e/evaluate_all.job
    Submitted batch job 4550234
    [minhle@int1 EvEn]$ tail -f output/e2e/evaluate_all.2018-08-03-7e6505c.sh.out
    ...
      ]
    }
    log_dir = "logs/best-inst-dev-2018-08-03-7e6505c"
    Loading word embeddings from glove.840B.300d.txt...

Forgot to set the `max_mention_width` parameter. Redo evaluation...

    [minhle@int1 EvEn]$ git pull
    [minhle@int1 EvEn]$ cd e2e-coref
    [minhle@int1 e2e-coref]$ git reset --hard
    [minhle@int1 e2e-coref]$ git pull
    [minhle@int1 e2e-coref]$ git checkout 23a9701
    Previous HEAD position was ffc3acc... add config
    HEAD is now at 23a9701... set max_mention_width
    [minhle@int1 e2e-coref]$ cd ..
    [minhle@int1 EvEn]$ python3 version.py
    2018-08-03-d9291d4
    [minhle@int1 EvEn]$ python3 scripts/e2e/gen_experiments_conf.py
    Wrote 56 configurations to e2e-coref/experiments.conf
    Wrote 56 commands to output/e2e/evaluate_all.2018-08-03-d9291d4.sh
    [minhle@int1 EvEn]$ git pull
    [minhle@int1 EvEn]$ python3 version.py
    2018-08-03-ad49615
    [minhle@int1 EvEn]$ sbatch scripts/e2e/evaluate_all.job
    Submitted batch job 4550316
    [minhle@int1 EvEn]$ tail -f output/e2e/evaluate_all.2018-08-03-d9291d4.sh.out
    Evaluating logs/best-inst-dev-2018-08-03-d9291d4/model.max.ckpt
    OOV rate for glove.840B.300d.txt: 1.48%
    OOV rate for turian.50d.txt: 1.99%
    Loaded 342 eval examples.
    ...

## Sat 4 Aug

Results:

    [minhle@int1 EvEn]$ grep -P "Running experiment|Average F1 \(conll\)" output/e2e/evaluate_all.2018-08-03-d9291d4.sh.out
    Running experiment: best-inst-dev-2018-08-03-d9291d4 (from command-line argument).
    Average F1 (conll): 58.39%
    Running experiment: best-inst-dev-gold-mention-spans-2018-08-03-d9291d4 (from command-line argument).
    Average F1 (conll): 74.11%
    Running experiment: best-inst-test-2018-08-03-d9291d4 (from command-line argument).
    Average F1 (conll): 58.29%
    Running experiment: best-inst-test-gold-mention-spans-2018-08-03-d9291d4 (from command-line argument).
    Average F1 (conll): 73.63%
    ...
    [minhle@int2 EvEn]$ python3 version.py
    2018-08-04-7d455fa
    [minhle@int2 EvEn]$ python3 scripts/e2e/extract_results.py
    Results written to output/e2e/results.2018-08-04-7d455fa.csv

## Sun 5 Aug 2018

Prepared scripts to retrain e2e-coref. Trying it out...

    [minhle@int2 EvEn]$ python3 version.py
    2018-08-05-225566b
    [minhle@int2 EvEn]$ python3 scripts/e2e/gen_experiments_conf.py
    Wrote 99 configurations to e2e-coref/experiments.conf
    Wrote 15 commands to output/e2e/train_all.2018-08-05-225566b.sh
    Wrote 84 commands to output/e2e/evaluate_all.2018-08-05-225566b.sh
    [minhle@int2 EvEn]$ output/e2e/train_all.2018-08-05-225566b.sh sbatch
    Submitted batch job 4552208
    Submitted batch job 4552209
    Submitted batch job 4552210
    Submitted batch job 4552211
    Submitted batch job 4552212
    Submitted batch job 4552213
    Submitted batch job 4552214
    Submitted batch job 4552215
    Submitted batch job 4552216
    Submitted batch job 4552217
    Submitted batch job 4552218
    Submitted batch job 4552219
    Submitted batch job 4552220
    Submitted batch job 4552221
    Submitted batch job 4552222
    [minhle@int2 EvEn]$ sbatch scripts/e2e/evaluate_all.job
    [minhle@int2 EvEn]$ tail -f output/e2e/evaluate_all.2018-08-05-225566b.sh.out

## Sun 19 Aug

e2e training and evaluation ran well but I will need to change them again (retrain on
gold, use gold mention spans for everything).

Implemented 3 baselines as discussed with Antske.

    [minhle@int2 EvEn]$ sbatch exp_baselines.job
    Submitted batch job 4594702
    [minhle@int2 EvEn]$ squeue | grep minh
            4594702    normal exp_base   minhle  R       0:01      1 tcn123
    
## Tue 21 Aug

I forgot to call the scorer. Rerun everything...

    [minhle@int1 EvEn]$ sbatch exp_baselines.job
    Submitted batch job 4601054
    [minhle@int1 EvEn]$ squeue | grep minh
            4601054    normal exp_base   minhle PD       0:00      1 (Resources)

Modified `cort` to use gold mention spans always. I want to try it out on Cartesius
(got a weird error on my Mac) but have to wait until the previous experiment starts.

## Wed 22 Aug

Looking back at previous evaluation results, interestingly, removing global matching
(with retraining) improves the results:

    Evaluating logs/best-inst-test-2018-08-03-d9291d4/model.max.ckpt
    Average F1 (conll): 58.29%

    Evaluating logs/best-inst_glob-test-2018-08-03-d9291d4/model.max.ckpt
    Average F1 (conll): 58.75%

Installed `cort` on Cartesius, trying out...
    (venv) [minhle@int1 EvEn]$ output/cort/venv/bin/cort-predict-conll -in data/conll-2012-flat/dev/bc_msnbc_0000.v4_auto_conll -model models/cort/model-latent-train.obj -out output/cort/dev_bc_msnbc_0000.v4_auto_conll.out -extractor cort.coreference.approaches.mention_pairs.extract_testing_substructures -perceptron cort.coreference.approaches.mention_pairs.MentionPairsPerceptron -clusterer cort.coreference.clusterer.best_first


    2018-08-22 06:05:41,156 INFO Reading in data.
    2018-08-22 06:05:50,557 INFO Extracting system mentions.
    2018-08-22 06:05:50,942 INFO Predicting.
    Reading corpus testing: 100%|########################################################| 21/21 [00:09<00:00,  2.25doc/s]
    2018-08-22 06:05:50,943 INFO    Removing coreference annotations from corpus.
    2018-08-22 06:05:50,943 INFO    Extracting instances and features.
    2018-08-22 06:05:56,209 INFO    Doing predictions.
    Traceback (most recent call last):
    File "output/cort/venv/bin/cort-predict-conll", line 4, in <module>
        __import__('pkg_resources').run_script('cort==0.2.4.5', 'cort-predict-conll')
    File "/nfs/home2/minhle/EvEn/output/cort/venv/lib/python3.5/site-packages/pkg_resources/__init__.py", line 719, in run_script
        self.require(requires)[0].run_script(script_name, ns)
    File "/nfs/home2/minhle/EvEn/output/cort/venv/lib/python3.5/site-packages/pkg_resources/__init__.py", line 1504, inrun_script
        exec(code, namespace, namespace)
    File "/nfs/home2/minhle/EvEn/output/cort/venv/lib/python3.5/site-packages/cort-0.2.4.5-py3.5.egg/EGG-INFO/scripts/cort-predict-conll", line 195, in <module>
        import_helper.import_from_path(args.clusterer)
    File "/nfs/home2/minhle/EvEn/output/cort/venv/lib/python3.5/site-packages/cort-0.2.4.5-py3.5.egg/cort/coreference/experiments.py", line 87, in predict
        arcs, labels, scores = perceptron.predict(substructures, arc_information)
    File "output/cort/venv/lib/python3.5/site-packages/cort-0.2.4.5-py3.5.egg/cort/coreference/perceptrons.pyx", line 231, in cort.coreference.perceptrons.Perceptron.predict
        substructure_arcs_scores, _, _, _, _) = self.argmax(
    File "/nfs/home2/minhle/EvEn/output/cort/venv/lib/python3.5/site-packages/cort-0.2.4.5-py3.5.egg/cort/coreference/approaches/mention_pairs.py", line 158, in argmax
        score_non_coref = self.score_arc(arc, arc_information, "-")
    File "output/cort/venv/lib/python3.5/site-packages/cort-0.2.4.5-py3.5.egg/cort/coreference/perceptrons.pyx", line 465, in cort.coreference.perceptrons.Perceptron.score_arc
        self.priors[label],
    KeyError: '-'

Fixed by adding '-' to `Perceptron.priors` and `Perceptron.weights` in 
`cort/coreference/perceptrons.pyx`. Evaluated: we got 100% precision which is good.
But we don't have 100% recall yet. Some mentions were ignored. Are they singletons?

    ====== TOTALS =======
    Identification of Mentions: Recall: (934 / 1228) 76.05% Precision: (934 / 934) 100%     F1: 86.4%
    --------------------------------------------------------------------------

    Coreference:
    Coreference links: Recall: (3334 / 5020) 66.41% Precision: (3334 / 4948) 67.38% F1: 66.89%
    --------------------------------------------------------------------------
    Non-coreference links: Recall: (29129 / 51294) 56.78%   Precision: (29129 / 30053) 96.92%       F1: 71.61%
    --------------------------------------------------------------------------
    BLANC: Recall: (0.616013304756565 / 1) 61.6%    Precision: (0.821530958201276 / 1) 82.15%       F1: 69.25%
    --------------------------------------------------------------------------

## Thu 6 Sep

Reran baseline experiments so that the output is written to a file:

    [minhle@int1 EvEn]$ python3 version.py
    2018-09-06-38bc48e
    [minhle@int1 EvEn]$ sbatch ./exp_baselines.job
    Submitted batch job 4646946

In parallel, working on processing the output captured in `slurm-4601054.out`.

Decided to refactor `exp_baselines.py`: move baseline definition to `baselines.py` and
execution to `scripts/run_baselines.py`. I would need to rerun the baseline.

Realized that I forgot to include the original set (no manipulation) in the list of 
manipulations so now I have to rerun everything from the beginning >"<

    [minhle@int2 EvEn]$ python3 version.py
    2018-09-06-b6d9abe
    [minhle@int2 EvEn]$ sbatch scripts/manipulate_corpus2.job
    Submitted batch job 4648361
    [minhle@int2 EvEn]$ squeue | grep minh
            4648361    normal manipula   minhle  R       0:27      1 tcn632

## Fri 7 Sep

Adapted `exp_coref_matrix` to run Stanford sieve instead of everything as the orginal plan.
Testing it out, I forgot how slow Stanford sieve is, which doesn't make sense: what does 
a rule-based system do that takes so much time?

Suddenly, I can't run any job on Cartesius at all. They always go into CG (completing) status
and die. I'm running low on budget too. Contacted SURFsara help desk.

It turns out that the two problems are related: my budget is not enough to run the job 
within the specified time. So now I have to wait for new budget (if that's possible).
In the meantime, I have Marieke's old laptop which I'll use as a mainframe.

## Sat 8 Sep

Tried to parrallelize computation using `joblib`. I should have done this earlier since it's
very easy and give an 24x boost on Cartesius (or even more because each of the 24 CPUs
is more performant than the ones on my laptop). Now I don't have budget to do anything any
more T_T

Can't run anything on Cartesius because of missing packages and
pip error `AttributeError: '_NamespacePath' object has no attribute 'sort'`. How frustrating!

Copying data to the loan laptop:

    sync-loanlap.sh

Zipping `output` folder on Cartesius so that it's easier to transfer it from Cartesius
to Kyoto and then down to the new lap. 
It takes a very loooong time. Now I see why people talk about bringing your code to
your data instead of the other way around.

I should start measuring the performance of MTurk workers now to see if they can meet 
our needs. If they annotate too poorly, we need to think about hiring students.
Especially the performance on truncated and masked text because it's hard to understand
shorter paragraphs. Hmmm... maybe I should actually hire students.

OK... finished copying everything to the loan laptop. Now I can start do things.

Fixed a bug caused by parallelism: if you don't set `-dcoref.conll.output` in CoreNLP,
multiple processes will write to the same path (output paths are timestamped but not 
precise enough to differentiate between processes started at the same time).
Things still run until the scorer is called on a broken file and fail, giving obscure
error messages.

Just running a script against a set of files is not trivial!

OK, got results of Stanford Sieve. Now, I'll come back to baselines:

    cltls-MacBook-Pro:EvEn minh$ git reset --hard
    HEAD is now at 76aaed1 parallel baseline ready
    cltls-MacBook-Pro:EvEn minh$ python3 version.py
    2018-09-08-76aaed1
    cltls-MacBook-Pro:EvEn minh$ nohup scripts/run_baselines.job &
    cltls-MacBook-Pro:EvEn minh$ watch 'tail -n 2 output/run_baselines.2018-09-08-76aaed1/*.log'
    ...
    cltls-MacBook-Pro:EvEn minh$ ls output/run_baselines.2018-09-08-76aaed1/*.log | wc -l
        99
    ... (still running)

While waiting for baselines to finish, now I'll work on `cort`. I meet my old acquaintance:

    ImportError: Building module cort.coreference.perceptrons failed: ["distutils.errors.CompileError: command 'clang' failed with exit status 1\n"]

This time I found out how to fix it :D Just run the following command:

    export CFLAGS=-I`python3 -c 'import numpy; print(numpy.get_include())'`

## Sun 9 Sep

Finished baseline experiments:

    cltls-MacBook-Pro:EvEn minh$ ls output/run_baselines.2018-09-08-76aaed1/*.log | wc -l
        180

Finished plotting baseline together with existing results.

Tried to train cort but got this weird error:

    Training pair on output/conll-2012-consolidated.2018-09-06-b3d6f1a/inst/train.m_gold_conll ...
    2018-09-09 11:35:53,091 INFO Reading in data.
    2018-09-09 11:44:28,659 INFO Extracting system mentions.
    2018-09-09 11:45:16,361 INFO Learning.
    2018-09-09 11:45:16,361 INFO 	Extracting instances and features.
    Process ForkPoolWorker-1:
    Traceback (most recent call last):
    File "/usr/local/Cellar/python/3.7.0/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/process.py", line 297, in _bootstrap
        self.run()
    File "/usr/local/Cellar/python/3.7.0/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/process.py", line 99, in run
        self._target(*self._args, **self._kwargs)
    File "/usr/local/Cellar/python/3.7.0/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/pool.py", line 110, in worker
        task = get()
    File "/usr/local/Cellar/python/3.7.0/Frameworks/Python.framework/Versions/3.7/lib/python3.7/multiprocessing/queues.py", line 354, in get
        return _ForkingPickler.loads(res)
    File "/Users/minh/EvEn/output/cort/venv/lib/python3.7/site-packages/nltk-3.3-py3.7.egg/nltk/tree.py", line 1070, in extend
        self._setparent(child, len(self))
    File "/Users/minh/EvEn/output/cort/venv/lib/python3.7/site-packages/nltk-3.3-py3.7.egg/nltk/tree.py", line 1224, in _setparent
        raise ValueError('Can not insert a subtree that already '
    ValueError: Can not insert a subtree that already has a parent.

Something is apparently wrong with NLTK -- it creates a tree that can be serialized but can't
be deserialized. I disabled multiprocessing in `cort.coreference.instance_extractors` (but 
there's still parallelism in the level of training multiple models).

Started training cort:

    cltls-MacBook-Pro:EvEn minh$ python3 version.py
    2018-09-09-fd6a19d
    cltls-MacBook-Pro:EvEn minh$ nohup scripts/cort/train_all.job &
    [1] 31391
    cltls-MacBook-Pro:EvEn minh$ tail -f nohup.out
    [Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.
    2018-09-09 22:19:10,855 INFO Reading in data.
    2018-09-09 22:19:10,861 INFO Reading in data.
    2018-09-09 22:19:10,893 INFO Reading in data.
    2018-09-09 22:19:10,893 INFO Reading in data.

Woke up and found that the machine was thrashing. In total, it was using just a little more
than the 8 GB RAM it has and 6 GB was decided to go into swap (why?) So the training that
took only 2 minutes on my computer now takes more than 5 hours.

Decided to decrease the number of processes by one. Retried. It still thrashed even though
it was using less than 8 GB, why? :-/ Use only 2 processes to see if anything better.
--> and it is better. Finished training 2 models in 30 min. With this rate we will cover all
the 15 manipulated corpora in 4 hr. Not bad at all.

Looked back at my attempts at retraining e2e, which were screwed up by Cartesius removing files
and some mistakes. Now I don't have much budget <s>but luckily, I could at least train on the
`gpu_short` partition which is off-budget.</s> 

Found out that I can use pip on Cartesius again, probably they fixed it without notifying me.
But now I can install a virtualenv, getting this error:

    Error: Command '['/nfs/home2/minhle/EvEn/output/cort/venv/bin/python3', '-Im', 'ensurepip', '--upgrade', '--default-pip']'returned non-zero exit status 1.

Now it works again 8-} Probably some problem with loading 3.6 after 3.5.2.

## Mon 10 Sep

It thrashed again while training the 3rd and 4th models. I'll try to run this job on 
`short` partition on Cartesius. Now I got into another problem: the file `perceptrons.pyx`
aren't compiled for some reason so I can't train any model:

    [minhle@int1 EvEn]$ python3 -c 'from cort.coreference import perceptrons'
    Traceback (most recent call last):
      File "<string>", line 1, in <module>
    ImportError: cannot import name 'perceptrons'

Because it keeps thrashing on the loan laptop and I keep having issues with running `cort`
on Cartesius, I'll train sequentially on the loan laptop. It might take 7 days but I can wait.

    cltls-MacBook-Pro:EvEn minh$ python version.py
    2018-09-10-1bfb759
    cltls-MacBook-Pro:EvEn minh$ nohup scripts/cort/train_all.job &
    [1] 34506
    cltls-MacBook-Pro:EvEn minh$ tail -f nohup.out           
    [Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.
    Training pair on output/conll-2012-consolidated.2018-09-06-b3d6f1a/inst/train.m_gold_conll ...
    2018-09-10 08:32:57,601 INFO Reading in data.
      6%|▌         | 159/2749 [00:32<08:41,  4.97document/s]

It's running steadily with the rate of 2 models/hr. So it will take roughly 2 days to finish
training 3 types of models on 15 manipulated corpora.

Come back to e2e:

    [minhle@int1 EvEn]$ python3 version.py
    2018-09-10-3edf0b6
    [minhle@int1 EvEn]$ `python3 scripts/e2e/gencmd_minimize_datasets.py`
    Wrote commands to output/e2e/minimize_datasets.2018-09-10-3edf0b6.sh
    ...
    [minhle@int1 EvEn]$ python3 version.py
    2018-09-10-eb865b6
    [minhle@int1 EvEn]$ `python3 scripts/e2e/gencmd_train_all.py` sbatch
    ...
    [minhle@int1 EvEn]$ sbatch scripts/e2e/evaluate_all.job

Fail and fail and fail... I have one mistake in `gencmd_train_all.py` file (linking
a gold-mention config with a pretrained model the authors provide).
Need to rerun everything again:

    [minhle@int1 EvEn]$ python3 version.py
    2018-09-10-26d2d90
    [minhle@int1 EvEn]$ `python3 scripts/e2e/gencmd_train_all.py` sbatch
    Wrote 76 configurations to e2e-coref/experiments.conf
    Wrote 16 commands to output/e2e/train_all.2018-09-10-26d2d90.sh
    Submitted batch job 4659625
    Submitted batch job 4659626
    Submitted batch job 4659627
    Submitted batch job 4659628
    Submitted batch job 4659629
    Submitted batch job 4659630
    Submitted batch job 4659631
    Submitted batch job 4659632
    Submitted batch job 4659633
    Submitted batch job 4659634
    Submitted batch job 4659635
    Submitted batch job 4659636
    Submitted batch job 4659637
    Submitted batch job 4659638
    Submitted batch job 4659639
    Submitted batch job 4659640

Looks like usage in `short` and `gpu_short` counts after all because my budget keeps bleeding
during Saturday and Sunday after I stopped submitting jobs to `normal`. Now I have barely any
amount left T_T

Apparently, there's some memory leakage problem with `cort` since the training gets slower and slower
and swap keeps growing. It manages to thrash with only one process. I have to change the code again,
avoiding running on the same process but spawning out a process for each dataset in the hope that
the memory will be cleaned once training finishes. This is as hard as it can get training
one system!

Saw an error with tqdm at the closing of the process:

    Exception ignored in: <function tqdm.__del__ at 0x1483102f0>
    Traceback (most recent call last):
    File "/Users/minh/EvEn/output/cort/venv/lib/python3.7/site-packages/tqdm/_tqdm.py", line 885, in __del__
        self.close()
    File "/Users/minh/EvEn/output/cort/venv/lib/python3.7/site-packages/tqdm/_tqdm.py", line 1090, in close
        self._decr_instances(self)
    File "/Users/minh/EvEn/output/cort/venv/lib/python3.7/site-packages/tqdm/_tqdm.py", line 454, in _decr_instances
        cls.monitor.exit()
    File "/Users/minh/EvEn/output/cort/venv/lib/python3.7/site-packages/tqdm/_monitor.py", line 52, in exit
        self.join()
    File "/usr/local/Cellar/python/3.7.0/Frameworks/Python.framework/Versions/3.7/lib/python3.7/threading.py", line 1029, in join
        raise RuntimeError("cannot join current thread")
    RuntimeError: cannot join current thread

It could be that the memory leak is caused by tqdm... Damn! Somehow it doesn't count to 100%
therefore it keeps everything in memory.

... and removing tqdm did solve the problem. The script worked for the first 10 models and then
tripped again:

    2018-09-10 19:57:49,116 INFO Started epoch 1
    Traceback (most recent call last):
    File "output/cort/venv/bin/cort-train", line 4, in <module>
        __import__('pkg_resources').run_script('cort==0.2.4.5', 'cort-train')
    File "/Users/minh/EvEn/output/cort/venv/lib/python3.7/site-packages/pkg_resources/__init__.py", line 658, in run_script
        self.require(requires)[0].run_script(script_name, ns)
    File "/Users/minh/EvEn/output/cort/venv/lib/python3.7/site-packages/pkg_resources/__init__.py", line 1438, in run_script
        exec(code, namespace, namespace)
    File "/Users/minh/EvEn/output/cort/venv/lib/python3.7/site-packages/cort-0.2.4.5-py3.7.egg/EGG-INFO/scripts/cort-train", line 141, in <module>
        perceptron
    File "/Users/minh/EvEn/output/cort/venv/lib/python3.7/site-packages/cort-0.2.4.5-py3.7.egg/cort/coreference/experiments.py", line 43, in learn
        perceptron.fit(substructures, arc_information)
    File "output/cort/venv/lib/python3.7/site-packages/cort-0.2.4.5-py3.7.egg/cort/coreference/perceptrons.pyx", line 182, in cort.coreference.perceptrons.Perceptron.fit
        self.__update(cons_arcs,
    File "output/cort/venv/lib/python3.7/site-packages/cort-0.2.4.5-py3.7.egg/cort/coreference/perceptrons.pyx", line 331, in cort.coreference.perceptrons.Perceptron.__update
        arc_information[arc][0]
    KeyError: None
    ...
    subprocess.CalledProcessError: Command '['output/cort/venv/bin/cort-train', '-in', '/Users/minh/EvEn/output/conll-2012-consolidated.2018-09-06-b3d6f1a/inst/train.m_gold_conll', '-out', '/Users/minh/EvEn/output/cort/train_all.2018-09-10/model-latent-inst.obj', '-extractor', 'cort.coreference.approaches.mention_ranking.extract_substructures', '-perceptron', 'cort.coreference.approaches.mention_ranking.RankingPerceptron', '-cost_function', 'cort.coreference.cost_functions.cost_based_on_consistency', '-cost_scaling', '100']' returned non-zero exit status 1.

The generated HTML files have the same speaker-mixup problem as we saw in Piek's class
(e.g. output/conll-2012-manipulated2.2018-09-06-b6d9abe/men_75/dev/bc_phoenix_00_phoenix_0000___part_005.m_gold_conll.html)

## Tue 11 Sep

I can't solve cort's issue right now, all I could do is rerun the training, trying to get as many
cases as possible.

    cltls-MacBook-Pro:EvEn minh$ python3 version.py
    2018-09-11-4fcb733
    cltls-MacBook-Pro:EvEn minh$ nohup scripts/cort/train_all.job 2 &
    [1] 40603

## Wed 12 Sep

All the latent and tree models have failed to train. I need to seriously look at this problem...

But for now, let's evaluate the models we already have...

    cltls-MacBook-Pro:EvEn minh$ python3 version.py
    2018-09-12-43abe48
    cltls-MacBook-Pro:EvEn minh$ nohup python3 -u scripts/cort/evaluate_all.py &
    [1] 50242
    cltls-MacBook-Pro:EvEn minh$ tail -f nohup.out
    Running "pair" on output/conll-2012-consolidated.2018-09-06-b3d6f1a/inst/dev.m_auto_conll ...
    2018-09-12 09:05:07,194 INFO Loading model.
    2018-09-12 09:05:11,290 INFO Reading in data.
    ...
    cltls-MacBook-Pro:EvEn minh$ python3 version.py
    2018-09-12-2f946f8
    cltls-MacBook-Pro:EvEn minh$ python3 scripts/cort/extract_results.py
    ...
    Results written to file output/cort/extract_results.2018-09-12-2f946f8.csv

Looked at the plots, we see a clear difference between retraining and no retraining.
Cort with retraining is doing much better than without so I guess when I finish evaluating
retrained e2e models, I'll see a big boost as well.

## Thu 13 Sep

When I downloaded the models from Cartesius to the loan lap, all the symbolic links are 
replaced by the actual folders so I had to check that they are indeed the same.

    cltls-MacBook-Pro:EvEn minh$ md5 e2e-coref/logs/eval-*-auto-2018-09-10-eb865b6-dev/model.max.ckpt.data*
    MD5 (e2e-coref/logs/eval-inst-auto-2018-09-10-eb865b6-dev/model.max.ckpt.data-00000-of-00001) = 0136d6325bb882642501636a037683e4
    MD5 (e2e-coref/logs/eval-inst_glob-auto-2018-09-10-eb865b6-dev/model.max.ckpt.data-00000-of-00001) = 18844da28d280f55b681bd1723896ba7
    MD5 (e2e-coref/logs/eval-men_1-auto-2018-09-10-eb865b6-dev/model.max.ckpt.data-00000-of-00001) = 18844da28d280f55b681bd1723896ba7
    MD5 (e2e-coref/logs/eval-men_100-auto-2018-09-10-eb865b6-dev/model.max.ckpt.data-00000-of-00001) = 18844da28d280f55b681bd1723896ba7
    MD5 (e2e-coref/logs/eval-men_20-auto-2018-09-10-eb865b6-dev/model.max.ckpt.data-00000-of-00001) = 18844da28d280f55b681bd1723896ba7
    MD5 (e2e-coref/logs/eval-men_5-auto-2018-09-10-eb865b6-dev/model.max.ckpt.data-00000-of-00001) = 18844da28d280f55b681bd1723896ba7
    MD5 (e2e-coref/logs/eval-men_50-auto-2018-09-10-eb865b6-dev/model.max.ckpt.data-00000-of-00001) = 18844da28d280f55b681bd1723896ba7
    MD5 (e2e-coref/logs/eval-men_75-auto-2018-09-10-eb865b6-dev/model.max.ckpt.data-00000-of-00001) = 18844da28d280f55b681bd1723896ba7
    MD5 (e2e-coref/logs/eval-nonmen_1-auto-2018-09-10-eb865b6-dev/model.max.ckpt.data-00000-of-00001) = 18844da28d280f55b681bd1723896ba7
    MD5 (e2e-coref/logs/eval-nonmen_100-auto-2018-09-10-eb865b6-dev/model.max.ckpt.data-00000-of-00001) = 18844da28d280f55b681bd1723896ba7
    MD5 (e2e-coref/logs/eval-nonmen_20-auto-2018-09-10-eb865b6-dev/model.max.ckpt.data-00000-of-00001) = 18844da28d280f55b681bd1723896ba7
    MD5 (e2e-coref/logs/eval-nonmen_5-auto-2018-09-10-eb865b6-dev/model.max.ckpt.data-00000-of-00001) = 18844da28d280f55b681bd1723896ba7
    MD5 (e2e-coref/logs/eval-nonmen_50-auto-2018-09-10-eb865b6-dev/model.max.ckpt.data-00000-of-00001) = 18844da28d280f55b681bd1723896ba7
    MD5 (e2e-coref/logs/eval-nonmen_75-auto-2018-09-10-eb865b6-dev/model.max.ckpt.data-00000-of-00001) = 18844da28d280f55b681bd1723896ba7
    MD5 (e2e-coref/logs/eval-orig-auto-2018-09-10-eb865b6-dev/model.max.ckpt.data-00000-of-00001) = 18844da28d280f55b681bd1723896ba7
    cltls-MacBook-Pro:EvEn minh$ md5 e2e-coref/logs/eval-*-gold-retrain-2018-09-10-eb865b6-dev/model.max.ckpt.data*
    MD5 (e2e-coref/logs/eval-inst-gold-retrain-2018-09-10-eb865b6-dev/model.max.ckpt.data-00000-of-00001) = 7d52461ae9c1614074a629fe1d0400cb
    MD5 (e2e-coref/logs/eval-inst_glob-gold-retrain-2018-09-10-eb865b6-dev/model.max.ckpt.data-00000-of-00001) = e6027c610065eb345e4136dfeaf23c3e
    MD5 (e2e-coref/logs/eval-men_1-gold-retrain-2018-09-10-eb865b6-dev/model.max.ckpt.data-00000-of-00001) = 6a1b58c391732d5a3379a2df501ffa50
    MD5 (e2e-coref/logs/eval-men_100-gold-retrain-2018-09-10-eb865b6-dev/model.max.ckpt.data-00000-of-00001) = f9cdc67498da0859222a13c3d81bf7bf
    MD5 (e2e-coref/logs/eval-men_20-gold-retrain-2018-09-10-eb865b6-dev/model.max.ckpt.data-00000-of-00001) = 03e037e6b8e71758e67291629dbaa57e
    MD5 (e2e-coref/logs/eval-men_5-gold-retrain-2018-09-10-eb865b6-dev/model.max.ckpt.data-00000-of-00001) = 45b6b324652489057e083753604d7320
    MD5 (e2e-coref/logs/eval-men_50-gold-retrain-2018-09-10-eb865b6-dev/model.max.ckpt.data-00000-of-00001) = 0cebd798817b152e5e24cdc213aa6a4f
    MD5 (e2e-coref/logs/eval-men_75-gold-retrain-2018-09-10-eb865b6-dev/model.max.ckpt.data-00000-of-00001) = b2e9309a7dc6706b7d9b4bda7d3cd272
    MD5 (e2e-coref/logs/eval-nonmen_1-gold-retrain-2018-09-10-eb865b6-dev/model.max.ckpt.data-00000-of-00001) = 2963bfbd650ca1258e14bc109e0867e8
    MD5 (e2e-coref/logs/eval-nonmen_100-gold-retrain-2018-09-10-eb865b6-dev/model.max.ckpt.data-00000-of-00001) = c39d3875504f345fb20e9b210f49f95d
    MD5 (e2e-coref/logs/eval-nonmen_20-gold-retrain-2018-09-10-eb865b6-dev/model.max.ckpt.data-00000-of-00001) = 3455ae2f600c484d1fff8e46e81e5cff
    MD5 (e2e-coref/logs/eval-nonmen_5-gold-retrain-2018-09-10-eb865b6-dev/model.max.ckpt.data-00000-of-00001) = bd4ec2c3b5c985a0a3bca4f6c51c26bc
    MD5 (e2e-coref/logs/eval-nonmen_50-gold-retrain-2018-09-10-eb865b6-dev/model.max.ckpt.data-00000-of-00001) = ea4f0a75866f9f984dd4d36300b111ac
    MD5 (e2e-coref/logs/eval-nonmen_75-gold-retrain-2018-09-10-eb865b6-dev/model.max.ckpt.data-00000-of-00001) = a3c6c311c11b6bf43c5ea3dca88054ce
    MD5 (e2e-coref/logs/eval-orig-gold-retrain-2018-09-10-eb865b6-dev/model.max.ckpt.data-00000-of-00001) = 983ac4b69f00f47dd6e4ea055d91bdc8

## Sun 16 Sep

Rerun e2e evaluation script because I used the wrong version.

    cltls-MacBook-Pro:EvEn minh$ nohup python3 -u scripts/e2e/evaluate_all.py &
    [1] 88277
    cltls-MacBook-Pro:EvEn minh$ tail -f nohup.out
    scripts/e2e/test-single.job eval-inst-auto-2018-09-10-eb865b6-dev &> output/e2e/evaluate_all-2018-09-16/eval-inst-auto-2018-09-10-eb865b6-dev.log

## Mon 17 Sep

Have a nice meeting with Piek and Antske. More ideas, TODOs, etc.
Now I need to prepare for some tasks that Piek and Antske could help with.

Implementing "external/internal knowledge" name transformations.
Trying out drake for data workflow management:

    Minhs-MacBook-Pro:EvEn cumeo$ drake
    The following steps will be run, in order:
    1: /Users/cumeo/Projects/spinoza/ulm-4/EvEn/././output/conll-2012-with-names-transformed.v1 <- /Users/cumeo/Projects/spinoza/ulm-4/EvEn/././data/conll-2012-flat [missing output]
    Confirm? [y/n] y
    Running 1 steps with concurrence of 1...

    --- 0. Running (missing output): /Users/cumeo/Projects/spinoza/ulm-4/EvEn/././output/conll-2012-with-names-transformed.v1 <- /Users/cumeo/Projects/spinoza/ulm-4/EvEn/././data/conll-2012-flat
    ...
    --- 0: /Users/cumeo/Projects/spinoza/ulm-4/EvEn/././output/conll-2012-with-names-transformed.v1 <- /Users/cumeo/Projects/spinoza/ulm-4/EvEn/././data/conll-2012-flat -> done in 7250.95s
    Done (1 steps run).

Looking at the data... I forgot to separate the 3 types of transformations into 3 sub folders
so they overwrote one another >"< Fine... it takes just 2 hours to rerun.

Implemented some statistics about names in CoNLL-2012. Rewrote Section 1 and Section 2.

## Mon 24 Sep 2018

Looking at missing results: 

    ==> output/e2e/evaluate_all-2018-09-16/eval-inst_glob-auto-2018-09-10-eb865b6-dev.log <==
    ==> output/e2e/evaluate_all-2018-09-16/eval-inst-auto-2018-09-10-eb865b6-test.log <==
    ==> output/e2e/evaluate_all-2018-09-16/eval-inst-auto-2018-09-10-eb865b6-dev.log <==

For some reason, the corresponding configurations are missing. To rerun things, I'm porting
the e2e section of README to the Drakefile. Rerunning things take a lot of time.
I'll write a new Cartesius proposal and fix the MTurk templates first.

Somehow, drake doesn't work together with `nohup`, I have to use screen instead. 

    cltls-MacBook-Pro:EvEn minh$ screen
    ... (drake -a)
    [detached]
    cltls-MacBook-Pro:EvEn minh$ tail -f output/e2e/train_all.v1.sh.log
    log_dir = "logs/train-gold-mention-spans"
    Setting CUDA_VISIBLE_DEVICES to: 1
    Loading word embeddings from glove.840B.300d.txt.filtered...
    Done loading word embeddings.
    Loading word embeddings from turian.50d.txt...
    Done loading word embeddings.
    [33600] loss=14.03, steps/s=525.20
    [33700] loss=15.99, steps/s=245.20
    [33800] loss=11.31, steps/s=166.05
    [33900] loss=15.89, steps/s=126.23
    ...

## Wed 26 Sep 2018
    
The training seems to have finished. 
Now it's time to evaluate. 

    cltls-MacBook-Pro:EvEn minh$ screen
    bash-3.2$ drake -a
    --- 4. Running (missing output): /Users/minh/EvEn/././output/e2e/evaluate_all.v1, /Users/minh/EvEn/././output/e2e/evaluate_all.v1.SUCCESS <- /Users/minh/EvEn/././e2e-coref/logs/train-orig-gold-retrain-2018-09-24
    Started: Wed Sep 26 08:21:50 CEST 2018
    scripts/setup-cartesius.sh: line 1: module: command not found
    scripts/setup-cartesius.sh: line 2: module: command not found
    scripts/setup-cartesius.sh: line 3: module: command not found
    [Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.
    ...
    [detached]

I'm doing this the
10th times, perhaps, because I want to have some code that's guaranteed to run.
It would be easier if I fix the models that fail only. Should I?

Found a bug again (of course!) This time, the transition from manual execution to drake
has failed me. No problem, restarting is easy :v

## Fri 28 Sep

One missing file (`minimized.../orig/test_auto...json`) means re-running the whole pipeline.
Luckily, it's as simple as running `drake` (and let it run for 3 days).

    cltls-MacBook-Pro:~ minh$ screen
    bash-3.2$ drake
    The following steps will be run, in order:  1: /Users/minh/EvEn/././output/e2e/conll-2012-minimized.v2, /Users/minh/EvEn/././output/e2e/conll-2012-minimized.v2.SUCCESS <-/Users/minh/EvEn/././output/conll-2012-consolidated.2018-09-06-b3d6f1a, /Users/minh/EvEn/././output/e2e/venv.v1 [missing output]
    2: /Users/minh/EvEn/././e2e-coref/logs/train-orig-gold-retrain-2018-09-28, /Users/minh/EvEn/././output/e2e/train_all.v2.SUCCESS <- /Users/minh/EvEn/././output/conll-2012-consolidated.2018-09-06-b3d6f1a, /Users/minh/EvEn/././output/e2e/conll-2012-minimized.v2 [projected timestamped]
    3: /Users/minh/EvEn/././output/e2e/evaluate_all.v3, /Users/minh/EvEn/././output/e2e/evaluate_all.v3.SUCCESS <- /Users/minh/EvEn/././e2e-coref/logs/train-orig-gold-retrain-2018-09-28 [projected timestamped]
    4: /Users/minh/EvEn/././output/e2e/results.v1.csv <- /Users/minh/EvEn/././output/e2e/evaluate_all.v3 [projected timestamped]
    Confirm? [y/n]
    ...
    [detached]

## Sun 30 Sep 2018

The steps have finished! It's super convenient with `drake`!

Because I switched to a different set of transformation, I'll need to rerun everything (yes, again!)
No problem, I have drake to do it for me. All I need to do now is to add one command 
(`replace_names.py`) to the transformation step:

    $[conll_2012_transformed], $[conll_2012_transformed].SUCCESS <- $[conll_2012_flat]
        python3 -u scripts/transformations/mask_mentions_or_context.py $INPUTS $OUTPUT0 && \
        python3 -u scripts/transformations/replace_names.py $INPUTS $OUTPUT0 && \
        touch $OUTPUT1

Look how neat it is!

I used this opportunity to move some of data preparation from the README file into Drakefile too.

## Mon 1 Oct

Talked to Antske about the graphical design of the MTurk experiment, these are some ideas
for improvement:

- Move "I didn't find..." button to more central position
- Make the instructions pane a different color
- Shorter passages (of course)
- A seperate task where people see a bigger passage or a whole document and  
connect clusters instead of mentions
- A "Done" button next to "I didn't find..." to stop linking (so the default action would
be to link mentions, without needing to hold Cmd)
- Exlain the goal in simple terms (e.g. we are looking to find out how people refer to things...)
- Avoid technical terms such as "mentions"
- Mark clusters different colors

## Mon 8 Oct

Upgraded the form, it looks much better and working with it feels more natural now.

I tried to use cookies to display instructions when the form is opened the first time only 
but solutions are either clunky (plain JS) or doesn't work (jquery-cookie). Will display
instructions always then.

Submitted a few test questions to MTurk sandbox (git commit fa328cd0933649977e3435f879b15a7a9548c8d2):

    Minhs-MacBook-Pro:EvEn cumeo$ drake -a output/mturk/chunk-submit.SUCCESS
    Running 2 steps with concurrence of 1...

    --- 13. Running (missing output): /Users/cumeo/Projects/spinoza/ulm-4/EvEn/././output/mturk/chunk, /Users/cumeo/Projects/spinoza/ulm-4/EvEn/././output/mturk/chunk.SUCCESS <- /Users/cumeo/Projects/spinoza/ulm-4/EvEn/././output/conll-2012-manipulated2.2018-04-13-271bf1e
    Processing /Users/cumeo/Projects/spinoza/ulm-4/EvEn/output/conll-2012-manipulated2.2018-04-13-271bf1e/men_5/train/bc_cctv_00_cctv_0003___part_000.m_auto_conll...
    Processing /Users/cumeo/Projects/spinoza/ulm-4/EvEn/output/conll-2012-manipulated2.2018-04-13-271bf1e/nonmen_1/train/bc_cctv_00_cctv_0003___part_001.m_auto_conll...
    Processing /Users/cumeo/Projects/spinoza/ulm-4/EvEn/output/conll-2012-manipulated2.2018-04-13-271bf1e/nonmen_20/train/bc_cctv_00_cctv_0003___part_002.m_auto_conll...
    Written 16 questions to output/mturk/chunk
    --- 13: /Users/cumeo/Projects/spinoza/ulm-4/EvEn/././output/mturk/chunk, /Users/cumeo/Projects/spinoza/ulm-4/EvEn/././output/mturk/chunk.SUCCESS <- /Users/cumeo/Projects/spinoza/ulm-4/EvEn/././output/conll-2012-manipulated2.2018-04-13-271bf1e -> done in 4.53s

    --- 14. Running (missing output): /Users/cumeo/Projects/spinoza/ulm-4/EvEn/././output/mturk/chunk-submit.SUCCESS <- /Users/cumeo/Projects/spinoza/ulm-4/EvEn/././output/mturk/chunk, /Users/cumeo/Projects/spinoza/ulm-4/EvEn/././output/mturk/chunk.SUCCESS
    Submitting HITs...
    10 ...
    Submitting HITs... Done.
    --- 14: /Users/cumeo/Projects/spinoza/ulm-4/EvEn/././output/mturk/chunk-submit.SUCCESS <- /Users/cumeo/Projects/spinoza/ulm-4/EvEn/././output/mturk/chunk, /Users/cumeo/Projects/spinoza/ulm-4/EvEn/././output/mturk/chunk.SUCCESS -> done in 6.15s
    Done (2 steps run).

## Mon 22 Oct

Meeting with Antske: Among other things, as soon as I get this paper in shape, I should 
think of the introduction and conclusion of my thesis.

Got a version of the design that I'm relatively happy with. Looked at the comments I got
from September last year and improve it more. I will ask CLTL members to annotate first.
I don't have the transformed corpora on Cartesius so I have to create them yet again:

    [minhle@tcn650 EvEn]$ module load python/3.5.2-intel-u2
    [minhle@tcn650 EvEn]$ drake output/conll-2012-transformed

This time it runs quite faster than before, looks like I'm using the full 24 CPUs because 
there's a big difference between wall time and CPU time.

    [minhle@int1 ~]$ squeue | grep minh
            4827290    normal     bash   minhle  R      17:50      1 tcn650
    [minhle@int1 ~]$ sacct --format="User,NodeList,CPUTime,State"
        User        NodeList    CPUTime      State
    --------- --------------- ---------- ----------
    minhle          tcn650   06:30:24    RUNNING

Looking for ways to solve conflicts in annotation data. Combed the 
[literature of mention-pair models](https://natural-language-understanding.wikia.com/wiki/Mention-pair_models_of_coreference_resolution?oldid=5928)
hoping to find some algorithms. The BestCut (or minimum cut in general) algorithm
seems good. Plan:

1. Turn clusters into positive and negative pairs
2. Detect clusters that have a conflict
3. Apply a minimum cut algorithm until no other conflict exists, ignore if 
it decreases the global score

## Tue 23 Oct

Found odd stuff in the data (repeated speakers):

    cltls-MacBook-Pro:EvEn minh$ grep ,Speaker data/conll-2012-flat/dev/* | wc -l
        44
    cltls-MacBook-Pro:EvEn minh$ grep ,Speaker data/conll-2012-flat/dev/* | head
    data/conll-2012-flat/dev/bc_cctv_0000.v4_auto_conll:bc/cctv/00/cctv_0000   4    0         The    DT  (TOP(FRAG(NP*       -   -   -   Speaker#6,Speaker#5,Speaker#5,Speaker#5    (FAC*   -
    data/conll-2012-flat/dev/bc_cctv_0000.v4_auto_conll:bc/cctv/00/cctv_0000   4    1         PLA   NNP      *       -   -   -   Speaker#6,Speaker#5,Speaker#5,Speaker#5        *   -
    data/conll-2012-flat/dev/bc_cctv_0000.v4_auto_conll:bc/cctv/00/cctv_0000   4    2        Hong   NNP      (NML(NML*       -   -   -   Speaker#6,Speaker#5,Speaker#5,Speaker#5        *   -

The job I started yesterday finished. The transformations were successfully
applied within 12 hours.

    [minhle@int1 EvEn]$ ls -ld output/conll-2012-transformed*
    drwxr-xr-x  2 minhle minhle 4096 Oct 23 01:00 output/conll-2012-transformed
    drwxr-xr-x 15 minhle minhle 4096 Oct 22 15:20 output/conll-2012-transformed-mentions-context
    -rw-r--r--  1 minhle minhle    0 Oct 22 23:09 output/conll-2012-transformed-mentions-context.SUCCESS
    drwxr-xr-x  5 minhle minhle 4096 Oct 22 23:09 output/conll-2012-transformed-names
    -rw-r--r--  1 minhle minhle    0 Oct 23 01:00 output/conll-2012-transformed-names.SUCCESS
    -rw-r--r--  1 minhle minhle    0 Oct 23 01:00 output/conll-2012-transformed.SUCCESS

Sent the tasks to CLTL mailing list

## Mon 29 Oct

To address Marten and Chantal's comments, I changed many things in 
the annotation tasks:

- Streamlined the workflow: less buttons, more intuitive
- Add functionality: remove a whole group
- Always enable submit button but provide error message when the form is incomplete

Besides, I also:

- Removed too short questions: make sure we always have the maximum number of sentences
in each question. The number of questions is reduced significantly: for my 9 sample
CoNLL documents, it goes from 53 down to 30 questions.
- Added annotation events so that we could do post-hoc analysis later
- Used cookies to hide instructions when the task is viewed repeatedly

Added details on the annotation tasks to my paper.

## Mon 5 Nov

Got some statistics about genres. I checked the correlation between 
token/name coverage and performance (see notebook `notebooks/name_coverage_vs_performance.ipynb`): 
found small positive correlation (0.2-0.3). Don't know what to do with this.

Found a problem: `auto_conll` files are missing from test corpora.
Fixed it but it means I'll need to rerun transformations yet again.

Talked to Antske. Promised to prepare the expert annotation task.

## Wed 7 Nov

Shortened name identifiers so that humans can work with them.

Prepared the expert annotation task and sent to Piek and Antske.

## Mon 26 Nov 2018

Discussed with Piek and Antske, we decided to go with student (+CLTL) annotations.

We also decided that we won't annotate documents where 100% of mentions or context
is masked because there wouldn't be any reasoning or understanding. Now I need to 
decide which point do I want to stop.

## Mon 4 Mar 2019

Prepared some documents for my first student assistent

```
In [12]: conf = ConfigFactory.parse_file('mturk/configs/cltl-2019-01-23-unmasked.conf')

In [13]: paths = conf.get_list('input_paths')

In [15]: import shutil, os

In [19]: [shutil.copy(path + '.html', '/Users/cumeo/Downloads/student/') for path in paths]
```

## 14 Mar 2019

Got a second student.

Need to change some aspects of the tool:

- display information to copy over to Google Form

## Sat 18 May 2019

OK, I've been recently pretty lazy about writing this lab log.
A lot of things have changed:

- We've got a third student
- They have annotated around 650 documents in 7-8 weeks
- We decided to do 80% and 100% masking too because we didn't find
the point where performance crashes yet
- We decided to change the story of the paper, from detailing a tool
to understanding the difference between humans and
the computer so that we know how to close the gap in performance
- That means we need to put a whole lot more thought into the 
design of the experiments. I'm going to meet a friend who does
psychology tomorrow to discuss methodology

A problem with the scores that I get so far is that small documents
contribute too much to the average (because it macro). Small documents
are noisy because you either get it all correct or you fail big time.
I checked [the standard evaluation tool](../data/conll-2012/scorer/v8.01/lib/CorScorer.pm)
and sure enough, they use micro average instead.
The problem with micro average, though, is that we can't compute 
standard deviation easily but we'll need to do subsampling.

## Sat 1 Jun 2019

OK... I'm getting really disorganized lately. Let's write more often.

While revising the paper, I realized that the named-entity _types_ should also be removed
to be realistic about what can be extracted from arbitrary tokens. While fixing that, I
realized that syntax and semantic roles weren't removed either due to a bug (the classic
modifying in place vs. re-assigning). All in all, that meant a day to fix bugs, add more
tests and restart all the jobs. Drake has been tremendously helpful so far because instead
waiting for one batch job to finish to start another (or using complicated bash script
to chain them), I have only one batch job that call `drake`.

As of now, I have results with `cort` using fixed tranformations. The accuracy decreased
slightly, barely noticeable with naked eyes.

I'm going to have an important meeting with Piek towards the end of this month (which is
also when the annotators finish their job). So I need to work harder on the paper now.

## Sun 2 Jun 2019

Got into a horrible TensorFlow error while trying to train e2e-coref:

    AttributeError: 'module' object has no attribute 'get_compile_flags'

It costed me half a day >"< I stopped short of reading through TensorFlow codebase to see
how they change the code during setup time dependent on the environment. At the end, I 
reverted my code to one of the revisions I noted down before and apparently it worked again.
(I should have thought of that sooner.)

## Thu 6 Jun 2019

Continue annotation efforts with an experiment on how well people can guess masked names
in a text. I'm running out of time now as the students only have a contract until June 30.

e2e training ran but somehow gives error while evaluating. It has something to do with
empty sentences in development set, if I'm not mistaken... I attempted to rerun setup 
in the hope that the problem will go away.

    [minhle@int2 EvEn]$ sbatch output/setup-e2e.job
    Submitted batch job 6365801
    [minhle@int2 EvEn]$ tail -f slurm-6365801.out 
    +++ SURFSARA_LIBRARY_PATH=/hpc/sw/cudnn-v6.0/cuda8.0.44/lib64:/hpc/sw/python-2.7.9/lib:/hpc/sw/python-2.7.9/lib/python2.7
    +++ export SURFSARA_LIBRARY_PATH
    + git reset --hard
    HEAD is now at 4bf2458 fix missing "wait" cmd, rerunning, adapting extract_results.py
    + git checkout 4bf245867a909a6725968bf190541f52d82dc991
    HEAD is now at 4bf2458... fix missing "wait" cmd, rerunning, adapting extract_results.py
    + scripts/e2e/setup.sh output/e2e/venv
    ...

No, it didn't change the development set. Actually, all the files are the same as they were
4 months ago.

    [minhle@int1 EvEn]$ md5sum e2e-coref/*.english.*conll
    b8aaf724fb5ac095a712d04cc0afd14f  e2e-coref/dev.english.v4_auto_conll
    6e64b649a039b4320ad32780db3abfa1  e2e-coref/test.english.v4_gold_conll
    6c33b3ab23a79772433c7a4f04997139  e2e-coref/train.english.v4_auto_conll

It's encoraging that I manage to reproduce this part exactly but it also means I didn't 
solve the evaluation error.

I accidentally sync the `e2e-coref` folder to Cartesius, overwriting everything my script
created a few hours ago :-( Rerunning now:

    [minhle@int1 EvEn]$ cat output/setup-e2e.job
    #!/bin/bash
    #SBATCH -t 1-00:00:00
    #SBATCH -p normal

    set -x

    echo -n "Started: " && date

    . scripts/setup-cartesius.sh

    git reset --hard && git checkout 4bf245867a909a6725968bf190541f52d82dc991 && scripts/e2e/setup.sh output/e2e/venv

    drake -a output/e2e/train_all.sh

    echo -n "Finished: " && date

    [minhle@int1 EvEn]$ git checkout 4bf245867a909a6725968bf190541f52d82dc991
    ...
    HEAD is now at 4bf2458... fix missing "wait" cmd, rerunning, adapting extract_results.py
    [minhle@int1 EvEn]$ sbatch output/setup-e2e.job
    Submitted batch job 6366024
    [minhle@int1 EvEn]$ tail -f slurm-6366024.out
    +++ SURFSARA_LIBRARY_PATH=/hpc/sw/cudnn-v6.0/cuda8.0.44/lib64:/hpc/sw/python-2.7.9/lib:/hpc/sw/python-2.7.9/lib/python2.7
    +++ export SURFSARA_LIBRARY_PATH
    + git reset --hard
    HEAD is now at 4bf2458 fix missing "wait" cmd, rerunning, adapting extract_results.py
    + git checkout 4bf245867a909a6725968bf190541f52d82dc991
    HEAD is now at 4bf2458... fix missing "wait" cmd, rerunning, adapting extract_results.py
    + scripts/e2e/setup.sh output/e2e/venv
    New python executable in /nfs/home2/minhle/EvEn/output/e2e/venv/bin/python2.7
    Not overwriting existing python script /nfs/home2/minhle/EvEn/output/e2e/venv/bin/python (you must use /nfs/home2/minhle/Ev

Sent name-guess questions to student2. If he runs out of tasks again, I'll make some
20%-masked documents for him.

## Tue 11 June 2018

Listened to how students do the task and I realize that I should have mask the speaker names
as well. Now, time to generate documents again...

Running e2e training again, hoping to get some results before the Monday meeting with Piek.
This time it actually runs:

    [100] loss=110.36, steps/s=4.27
    [200] loss=78.46, steps/s=4.47
    [300] loss=77.17, steps/s=4.54
    [400] loss=60.82, steps/s=4.76

According to the authors, it "generally converges at about 400k steps". I have only trained
each model for 1 hour so far but this time I'll do 24 hours to follow their hint.

But before doing so, I'll train 3 models quickly to get some preliminary results.

    [minhle@int1 EvEn]$ sbatch scripts/e2e/train-single.job train-no-external-gold-retrain-2019-06-06
    Submitted batch job 6402420
    [minhle@int1 EvEn]$ sbatch scripts/e2e/train-single.job train-no-internal-gold-retrain-2019-06-06
    Submitted batch job 6402430
    [minhle@int1 EvEn]$ sbatch scripts/e2e/train-single.job train-no-name-gold-retrain-2019-06-06
    Submitted batch job 6402431
    [minhle@tcn859 EvEn]$ sbatch scripts/e2e/train-single.job train-orig-gold-retrain-2019-06-06
    Submitted batch job 6402455

I wrote down this hypothesis in my paper draft:

    \hypothesis{Neural models show a difference between \m{No-Internal} and \m{No-External} (a sign of memorization) and/or a reduced difference between \m{Original} and \m{No-Match} (a sign of neglection).}

Running evaluation:

    python3 -u scripts/e2e/evaluate_all.py --suffix=2019-06-06 '--pattern=(no-internal|no-external|no-name)-gold-retrain' output/e2e/eval-models-trained-1hr
    python3 -u scripts/e2e/evaluate_all.py --suffix=2019-06-06 '--pattern=orig-gold-retrain' output/e2e/eval-models-trained-1hr

Getting results:

    [minhle@tcn1317 EvEn]$ grep "Average F1 (conll)" output/e2e/eval-models-trained-1hr/*
    output/e2e/eval-models-trained-1hr/eval-no-external-gold-retrain-2019-06-06-dev.log:Average F1 (conll): 81.80%
    output/e2e/eval-models-trained-1hr/eval-no-internal-gold-retrain-2019-06-06-dev.log:Average F1 (conll): 81.49%
    output/e2e/eval-models-trained-1hr/eval-no-name-gold-retrain-2019-06-06-dev.log:Average F1 (conll): 75.54%
    output/e2e/eval-models-trained-1hr/eval-orig-gold-retrain-2019-06-06-dev.log:Average F1 (conll): 84.36%

Submit jobs training e2e on all variations of the corpus:

    [minhle@int1 EvEn]$ bash output/e2e/train_all.sh sbatch
    Submitted batch job 6402559
    Submitted batch job 6402560
    Submitted batch job 6402561
    Submitted batch job 6402562
    Submitted batch job 6402563
    Submitted batch job 6402564
    Submitted batch job 6402565
    Submitted batch job 6402566
    Submitted batch job 6402567
    Submitted batch job 6402568
    Submitted batch job 6402569
    Submitted batch job 6402570
    Submitted batch job 6402571
    Submitted batch job 6402572
    Submitted batch job 6402573
    [minhle@int1 EvEn]$ squeue | grep minh  
            6402564       gpu train-si   minhle  R       0:17      1 gcn19
            6402565       gpu train-si   minhle  R       0:17      1 gcn20
            6402566       gpu train-si   minhle  R       0:17      1 gcn21
            6402567       gpu train-si   minhle  R       0:17      1 gcn22
            6402568       gpu train-si   minhle  R       0:17      1 gcn23
            6402569       gpu train-si   minhle  R       0:17      1 gcn24
            6402570       gpu train-si   minhle  R       0:17      1 gcn25
            6402571       gpu train-si   minhle  R       0:17      1 gcn10
            6402572       gpu train-si   minhle  R       0:17      1 gcn11
            6402573       gpu train-si   minhle  R       0:17      1 gcn12
            6402559       gpu train-si   minhle  R       0:20      1 gcn32
            6402560       gpu train-si   minhle  R       0:20      1 gcn64
            6402561       gpu train-si   minhle  R       0:20      1 gcn43
            6402562       gpu train-si   minhle  R       0:20      1 gcn44
            6402563       gpu train-si   minhle  R       0:20      1 gcn45
            
By the end of tomorrow, I will have used a big chunk of my budget.

I wrote this command to peek at the results:

    [minhle@int1 EvEn]$ find slurm-64025* -exec sh -c 'echo "{}: "; grep "log_dir" {} | tail -n 1;  grep "Current max F1" {} | tail -n 1' \;

## Sun 16 Jun 2019

Training has finished:

    [minhle@int2 EvEn]$ find slurm-64025* -exec sh -c 'echo "{}: "; grep "log_dir" {} | tail -n 1;  grep "Current max F1" {} | tail -n 1' \;
    slurm-6402559.out: 
    log_dir = "logs/train-orig-auto-2019-06-06"
    slurm-6402560.out: 
    log_dir = "logs/train-men_100-gold-retrain-2019-06-06"
    Current max F1: 54.84
    slurm-6402561.out: 
    log_dir = "logs/train-men_20-gold-retrain-2019-06-06"
    Current max F1: 74.56
    slurm-6402562.out: 
    log_dir = "logs/train-men_40-gold-retrain-2019-06-06"
    Current max F1: 66.86
    slurm-6402563.out: 
    log_dir = "logs/train-men_60-gold-retrain-2019-06-06"
    Current max F1: 61.12
    slurm-6402564.out: 
    log_dir = "logs/train-men_80-gold-retrain-2019-06-06"
    Current max F1: 57.01
    slurm-6402565.out: 
    log_dir = "logs/train-no-external-gold-retrain-2019-06-06"
    Current max F1: 82.36
    slurm-6402566.out: 
    log_dir = "logs/train-no-internal-gold-retrain-2019-06-06"
    Current max F1: 82.28
    slurm-6402567.out: 
    log_dir = "logs/train-no-name-gold-retrain-2019-06-06"
    Current max F1: 76.61
    slurm-6402568.out: 
    log_dir = "logs/train-nonmen_100-gold-retrain-2019-06-06"
    Current max F1: 85.29
    slurm-6402569.out: 
    log_dir = "logs/train-nonmen_20-gold-retrain-2019-06-06"
    Current max F1: 85.31
    slurm-6402570.out: 
    log_dir = "logs/train-nonmen_40-gold-retrain-2019-06-06"
    Current max F1: 85.31
    slurm-6402571.out: 
    log_dir = "logs/train-nonmen_60-gold-retrain-2019-06-06"
    Current max F1: 85.12
    slurm-6402572.out: 
    log_dir = "logs/train-nonmen_80-gold-retrain-2019-06-06"
    Current max F1: 84.86
    slurm-6402573.out: 
    log_dir = "logs/train-orig-gold-retrain-2019-06-06"
    Current max F1: 85.63

I suspect that there's a difference between `no-match` and `no-name` so I created
a new `no-match` corpus (each occurence of a name is mapped to a unique token) and
run training:

    [minhle@int2 EvEn]$ sbatch scripts/e2e/train-single.job train-no-match-gold-retrain-2019-06-06 ""
    Submitted batch job 6403157

    [minhle@int2 EvEn]$ tail slurm-6403157.out 
    Predicted conll file: /scratch-local/minhle.6403157/tmpT9o0eD
    [32500] loss=21.32, steps/s=5.96
    Average F1 (conll): 75.90%
    Average F1 (py): 75.91%
    Average precision (py): 78.38%
    Average recall (py): 74.04%
    Current max F1: 75.90

Attempting to evaluate models that are not retrained:

    [minhle@int2 EvEn]$ sbatch scripts/run-steps-on-cartesius1.job 
    Submitted batch job 6403179

All configs failed because they share a model that had failed to train on `orig` data.
I need to redirect it to the pretrained model at `final`:

    [minhle@int2 EvEn]$ rm -rf /nfs/home2/minhle/EvEn/e2e-coref/logs/train-orig-auto-2019-06-06
    [minhle@int2 EvEn]$ ln -s `pwd`/e2e-coref/logs/final /nfs/home2/minhle/EvEn/e2e-coref/logs/train-orig-auto-2019-06-06
    [minhle@int2 EvEn]$ sbatch scripts/run-steps-on-cartesius1.job 
    Submitted batch job 6403194

But that failed too because the pretrained model used some different dimensions.
This is the error I get when training my model:

    2019-06-17 01:32:35.127137: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K40m, pci bus id: 0000:83:00.0)
    Exception in thread Thread-5:
    Traceback (most recent call last):
    File "/hpc/sw/python-2.7.9/lib/python2.7/threading.py", line 810, in __bootstrap_inner
        self.run()
    File "/hpc/sw/python-2.7.9/lib/python2.7/threading.py", line 763, in run
        self.__target(*self.__args, **self.__kwargs)
    File "/nfs/home2/minhle/EvEn/e2e-coref/coref_model.py", line 65, in _enqueue_loop
        tensorized_example = self.tensorize_example(example, is_training=True)
    File "/nfs/home2/minhle/EvEn/e2e-coref/coref_model.py", line 96, in tensorize_example
        max_word_length = max(max(max(len(w) for w in s) for s in sentences), max(self.config["filter_widths"]))
    File "/nfs/home2/minhle/EvEn/e2e-coref/coref_model.py", line 96, in <genexpr>
        max_word_length = max(max(max(len(w) for w in s) for s in sentences), max(self.config["filter_widths"]))
    ValueError: max() arg is an empty sequence

Found the error: for whatever reason, the `*.jsonlines` files built by e2e itself are broken.
I replace them with those compiled from my `orig` corpus and things work again.

    [minhle@int2 EvEn]$ sbatch scripts/e2e/train-single.job train-orig-auto-2019-06-06 ""
    Submitted batch job 6403198
    [minhle@int2 EvEn]$ tail -f slurm-6403198.out 
    [200] loss=64.72, steps/s=5.05
    [300] loss=63.07, steps/s=5.06
    [400] loss=53.00, steps/s=5.07

While it's training, I'll peek at some results:

    [minhle@int2 EvEn]$ sbatch scripts/run-steps-on-cartesius1.job
    Submitted batch job 6403203
    [minhle@int2 EvEn]$ tail -f slurm-6403203.out 

## Sun 28 Jul 2019

I started training and testing `cort` again on Cartesius, this time using a combination
of `dev` and `test` data. This is to make it more compatible with human annotations.
Re-training is superfluous, I know; I did it just because it's less error-prone to 
rerun the whole thing using `drake`.

    [minhle@int1 EvEn]$ sbatch scripts/run-steps-on-cartesius1.job
    [minhle@int1 EvEn]$ tail -f slurm-6560720.out
    ...
    2019-07-28 22:42:55,335 INFO Extracting system mentions.
    2019-07-28 22:43:07,805 INFO Predicting.
    2019-07-28 22:43:07,805 INFO 	Removing coreference annotations from corpus.
    2019-07-28 22:43:07,838 INFO 	Extracting instances and features.

For e2e, I reran evaluation only, scheduled to run after new config files are generated:

    [minhle@int1 EvEn]$ rm -rf output/e2e/eval
    [minhle@int1 EvEn]$ sbatch --dependency 6560720 scripts/run-steps-on-cartesius3.job
    Submitted batch job 6560840

Slowly recalling how I trained `deep-coref`... It's a mixture of Java and Python...
TODO for tomorrow: 

- install `deep-coref` on Cartesius
- train one model
- check if I'm using the right paths

## Mon 29 Jul

Extracting results failed for e2e because I didn't update the file path pattern
(from `dev|test` to `dev_test`). Rerunning it is easy though because Drake has taken care 
of everything :D

Getting baseline results by adding a command into a script and rerun it:

    [minhle@int2 EvEn]$ srun scripts/run-steps-on-cartesius1.job

Preparing the input to deep-coref training: it's mostly about linking files, calling scripts
to transform the files from one format to another -- not that different from what I do at
my company. Who said that academia is more interesting?

    [minhle@int1 EvEn]$ sbatch scripts/run-steps-on-cartesius1.job
    Submitted batch job 6571200
    [minhle@int1 EvEn]$ tail -f slurm-6571200.out
    [main] INFO edu.stanford.nlp.coref.neural.NeuralCorefDataExporter - Processed document 1 in 1.626s
    [main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... done [1.2 sec].
    [main] INFO edu.stanford.nlp.coref.neural.NeuralCorefDataExporter - Processed document 0 in 0.585s
    [main] INFO edu.stanford.nlp.pipeline.MentionAnnotator - Using mention detector type: rule

## Tue 30 July

Attempted to train deep-coref but got into problem with theano.
When I installed Theano on Python 2.7.9 on Cartesius, it doesn't work (it did before).
It works on Python 3 but it's not an option because the code base is written in Python 2
(lots of not-bracketed `print` and other hidden issues).

## Thu 1 Aug

I realized that the problem was due to virtualenv. When I moved back to bare Python,
things worked again. I ran into another problem with the folder structure with only
`train` and `dev_test`. It's better to keep the normal train/dev/test but convert
`test` into the combination of dev and test.

## Mon 26 Aug

Came back from my holidays. The Cartesisus project is almost over, I asked them to give me
another 2 months. But in any case, I need to hurry up.

Replaced `dev_test` with `test` in `scripts/deep-coref/join_dev_test.py`. This doesn't affect
other systems. Started generating `output/deep-coref/conll-2012-transformed-exported`:

    [minhle@int2 EvEn]$ sbatch scripts/run-steps-on-cartesius1.job
    Submitted batch job 6770723

Tried training a model and got into a problem again: `deep-coref` insists on using a development
set so I had to extract 10% of the training set to become a development set. 

Ran training on nonmen_100 and got this error: 

    ...
      File "/nfs/home2/minhle/EvEn/deep-coref/datasets.py", line 309, in __init__
        for ana in range(0, me - ms)])
    ValueError: need at least one array to concatenate

I'll try other versions... Trying training again:

    [minhle@int1 EvEn]$ output/deep-coref/train_all.sh sbatch
    Submitted batch job 6772970

Same error, it turns out that sometimes, documents don't have any mentions and break the code.
I checked that the CoNLL files all have at least one mentions so they must have lost 
somewhere in between... It must have been in CoreNLP/deep-coref. I added a check to solve 
the error. Need to try again now, just building the corpora takes 40 mins :-O Unbelievable...

Running past the previous point:

    DEBUGGING: docs_path=/nfs/home2/minhle/EvEn/output/deep-coref/conll-2012-transformed-exported/men_20/features/doc_data/train/
    DEBUGGING: docs_path=/nfs/home2/minhle/EvEn/output/deep-coref/conll-2012-transformed-exported/men_20/features/doc_data/dev/
    Building model
    EPOCH 1, model = /nfs/home2/minhle/EvEn/output/deep-coref/conll-2012-transformed-exported/men_20/models/all_pairs/
    Training
    102/4058 [..............................] - ETA: 17571s - train loss: 0.1401^C

But it's not using the GPU. Canceled the job.

## Tue 27 Aug 2019

Added a flag to enable GPU usage, restarted...

Had a short meeting with Antske, the plan is the same.

Trying to train models on a CPU... I'll need to see how high of a performance it can reach
within 5 days:

    [minhle@int1 EvEn]$ sbatch scripts/deep-coref/train-all-cartesius-cpu.job
    Submitted batch job 6773977
    [minhle@int1 EvEn]$ tail -f slurm-6773977.out
    NVIDIA: no NVIDIA devices found
    NVIDIA: no NVIDIA devices found
    NVIDIA: no NVIDIA devices found
    NVIDIA: no NVIDIA devices found
    Adding sources from train
    Adding sources fromAdding sources fromAdding sources from  train train
    train

    Adding sources from train
    75/2467 [..............................] - ETA: 44s  71/2467 [..............................] - ETA: 40s  99/2468 [>.............................] - ETA: 59s

## Fri 30 Aug 2019

Trying out conda on Cartesius following the advice of a SurfSARA admin, it seems to work...

Spent another evening figuring out the right version of things via trial-and-error.
Right now, I'm settling with this config:

    module load Miniconda2
    create --name python2-conda python=2.7
    source activate python2-conda
    conda install mkl-service theano=0.8 pygpu=0.7.5

    module load cuda/7.0.28
    module load cudnn/7.0-v4-prod
    source activate python2-conda
    export THEANO_FLAGS='device=gpu,floatX=float32'

Every single line must be used exactly otherwise an error will occur. This is unbelievable...
I'll swear I will note down the version of every single package I use from now on.

IT RUN!!! FINALLYYYYYY!

    [minhle@int1 EvEn]$ cat output/deep-coref/train_all.sh
    BASH=${1-bash}
    $BASH scripts/deep-coref/train-single.job /nfs/home2/minhle/EvEn/output/deep-coref/conll-2012-transformed-exported/no-name

    [minhle@int1 EvEn]$ output/deep-coref/train_all.sh sbatch
    Submitted batch job 6783939
    [minhle@int1 EvEn]$ tail -f slurm-6783939.out
    ++ unset CONDA_PATH
    ++ [[ -n 4.2.46(2)-release ]]
    ++ hash -r
    + export THEANO_FLAGS=device=gpu,floatX=float32
    + THEANO_FLAGS=device=gpu,floatX=float32
    + cd deep-coref
    + python run_all.py /nfs/home2/minhle/EvEn/output/deep-coref/conll-2012-transformed-exported/no-name
    Using gpu device 0: Tesla K40m (CNMeM is disabled, cuDNN 4007)
    /home/minhle/.conda/envs/python2-conda/lib/python2.7/site-packages/theano/tensor/signal/downsample.py:6: UserWarning: downsample module has been moved to the theano.tensor.signal.pool module.
    "downsample module has been moved to the theano.tensor.signal.pool module.")
    ...

## Sat 31 Aug 2019

Found another trap: if I start a SLURM job from a Conda-activated environment, modules won't
be loaded properly and GPUs won't be used. I had to add this line to SLURM jobs to make sure
variables of the calling environment won't propagate to the job:

    #SBATCH --export=NONE

After adding the line, things work again.

Try training two models at the same time:

    [minhle@int1 EvEn]$ tail -n 100 slurm-6786457.out
    ...
    EPOCH 54, model = /nfs/home2/minhle/EvEn/output/deep-coref/conll-2012-transformed-exported/men_60/models/all_pairs/
    2254/2991 [=====================>........] - ETA: 63s - train loss: 0.0696Training
    2991/2991 [==============================] - 258s - train loss: 0.0714   86
    Testing on dev set
    343/343 [==============================] - 8s - anaphoricity loss: 0.2778 - loss: 0.0751     icity loss: 0.2945 - loss: 0.0707
    693/3325 [=====>........................] - ETA: 273s - train loss: 0.0284dev anaphoricity - loss: 0.2778 - auc: 0.9429 - f1: 0.8772 (thresh=0.45)
    746/3325 [=====>........................] - ETA: 267s - train loss: 0.0283dev - loss: 0.0751 - auc: 0.8061 - f1: 0.7391 (thresh=0.50)
    Times: compile: 16.1, preprocess_dataset: 105.9, train: 20547.3, metrics: 415.9, minibatch_prep: 1139.2

    EPOCH 76, model = /nfs/home2/minhle/EvEn/output/deep-coref/conll-2012-transformed-exported/nonmen_100/models/all_pairs/
    772/3325 [=====>........................] - ETA: 263s - train loss: 0.0286Training
    2991/2991 [==============================] - 258s - train loss: 0.0712    - ETA: 14s - train loss: 0.02813ss: 0.0717 - ETA: 92s - train loss: 0.0285Testing on dev set
    3325/3325 [==============================] - 343s - train loss: 0.0281   .2899 - loss: 0.0580
    259/343 [=====================>........] - ETA: 1s - anaphoricity loss: 0.2894 - loss: 0.0586Testing on dev set
    268/343 [======================>.......] - ETA: 1s - anaphoricity loss: 0.2851 - loss: 0.0648[minhle@int1 EvEn]$

Two GPUs are utilized at the same time:

    [minhle@gcn60 ~]$ nvidia-smi
    Sun Sep  1 00:05:29 2019
    +-----------------------------------------------------------------------------+
    | NVIDIA-SMI 418.39       Driver Version: 418.39       CUDA Version: 10.1     |
    |-------------------------------+----------------------+----------------------+
    | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
    | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
    |===============================+======================+======================|
    |   0  Tesla K40m          On   | 00000000:02:00.0 Off |                    0 |
    | N/A   39C    P0   105W / 235W |    365MiB / 11441MiB |     77%      Default |
    +-------------------------------+----------------------+----------------------+
    |   1  Tesla K40m          On   | 00000000:82:00.0 Off |                    0 |
    | N/A   39C    P0    91W / 235W |    151MiB / 11441MiB |     72%      Default |
    +-------------------------------+----------------------+----------------------+

    +-----------------------------------------------------------------------------+
    | Processes:                                                       GPU Memory |
    |  GPU       PID   Type   Process name                             Usage      |
    |=============================================================================|
    |    0     24509      C   python                                       354MiB |
    |    1     24510      C   python                                       140MiB |
    +-----------------------------------------------------------------------------+

Question: does the training create a `reward_rescaling` next to `all_pairs` folder? 
I suspect `all_pairs` is for the pre-training (150 epochs), once it finished that, it will
create the other folder to store the actual model.

    [minhle@int1 EvEn]$ ls /nfs/home2/minhle/EvEn/output/deep-coref/conll-2012-transformed-exported/men_60/models/
    all_pairs

## Sun 1 Sep 2019

... and it did.

    [minhle@gcn60 ~]$ ls /nfs/home2/minhle/EvEn/output/deep-coref/conll-2012-transformed-exported/men_60/models/
    all_pairs  reward_rescaling  top_pairs

Results are coming:

    [minhle@gcn60 EvEn]$ scripts/deep-coref/eval-single-python-model.sh output/deep-coref/conll-2012-transformed-exported/men_60/
    Using gpu device 0: Tesla K40m (CNMeM is disabled, cuDNN 4007)
    Building model
    Setting weights from reward_rescaling
    output/deep-coref/conll-2012-transformed-exported/men_60/models/reward_rescaling/best_weights.hdf5
    Evaluating model on test
    933/933 [==============================] - 70s - loss: 1.0898
    Writing output
    test - MUC: 31.27 - B3: 19.48 - CEAFE: 18.67 - LEA 14.25 - CoNLL 23.14
    test - loss: 1.0898 - CN: 6905 - CL: 37460 - FN: 530 - FL: 14561 - WL: 3237
        ranking: 0.9205 - anaphoricity: 0.4778

    [minhle@gcn60 EvEn]$ scripts/deep-coref/eval-single-python-model.sh output/deep-coref/conll-2012-transformed-exported/nonmen_100/
    Using gpu device 0: Tesla K40m (CNMeM is disabled, cuDNN 4007)
    Building model
    Setting weights from reward_rescaling
    output/deep-coref/conll-2012-transformed-exported/nonmen_100/models/reward_rescaling/best_weights.hdf5
    Evaluating model on test
    819/819 [==============================] - 54s - loss: 0.5687
    Writing output
    test - MUC: 78.18 - B3: 67.68 - CEAFE: 65.61 - LEA 64.00 - CoNLL 70.49
    test - loss: 0.5687 - CN: 19672 - CL: 23837 - FN: 1672 - FL: 1634 - WL: 1832
        ranking: 0.9286 - anaphoricity: 0.9225

They are quite lower than other systems but still make sense. 
**Notice that this is just the first few epochs, results will go up over time**

Now it's time to train all models at once. I decided to train 4 models on 
one Cartesius node after observing that both processing and memory utilization is
low on CPU and GPU.

    [minhle@int1 EvEn]$ drake +=output/deep-coref/train_all.sh
    The following steps will be run, in order:
    1: /nfs/home2/minhle/EvEn/././output/deep-coref/train_all.sh <- /nfs/home2/minhle/EvEn/././output/deep-coref/conll-2012-transformed-exported [forced]
    Confirm? [y/n] y
    Running 1 steps with concurrence of 1...

    --- 26. Running (forced): /nfs/home2/minhle/EvEn/././output/deep-coref/train_all.sh <- /nfs/home2/minhle/EvEn/././output/deep-coref/conll-2012-transformed-exported
    Wrote 16 commands to /nfs/home2/minhle/EvEn/output/deep-coref/train_all.sh
    --- 26: /nfs/home2/minhle/EvEn/././output/deep-coref/train_all.sh <- /nfs/home2/minhle/EvEn/././output/deep-coref/conll-2012-transformed-exported -> done in 0.15s
    Done (1 steps run).
    [minhle@int1 EvEn]$ output/deep-coref/train_all.sh sbatch
    Submitted batch job 6789464
    Submitted batch job 6789465
    Submitted batch job 6789466
    Submitted batch job 6789467

Looking at one of the jobs:

    [minhle@int1 EvEn]$ tail -f slurm-6789464.out
    + python -u run_all.py /nfs/home2/minhle/EvEn/output/deep-coref/conll-2012-transformed-exported/men_100
    + '[' '!' -z /nfs/home2/minhle/EvEn/output/deep-coref/conll-2012-transformed-exported/men_40 ']'
    + THEANO_FLAGS=device=gpu1,floatX=float32
    + python -u run_all.py /nfs/home2/minhle/EvEn/output/deep-coref/conll-2012-transformed-exported/men_20
    + '[' '!' -z /nfs/home2/minhle/EvEn/output/deep-coref/conll-2012-transformed-exported/men_60 ']'
    + THEANO_FLAGS=device=gpu0,floatX=float32
    + python -u run_all.py /nfs/home2/minhle/EvEn/output/deep-coref/conll-2012-transformed-exported/men_40
    + wait
    + THEANO_FLAGS=device=gpu1,floatX=float32
    + python -u run_all.py /nfs/home2/minhle/EvEn/output/deep-coref/conll-2012-transformed-exported/men_60
