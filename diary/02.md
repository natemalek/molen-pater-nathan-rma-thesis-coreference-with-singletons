## Tue 25 Apr

I found [162 cases of precision errors of "you"](you-precision-errors/error_analysis.html) 
in dev set. Not all of them is
interesting or solvable but it was easier to find an example than
with previous phenomena. Not only pleonastic cases are hard but also cases 
where someone not among the speakers say something or are talked to. 

This is the code I used to extract errors:

```
import cort
from cort.core import corpora
from cort.analysis import error_extractors
from cort.analysis import spanning_tree_algorithms
import sys
sys.argv = ' data/conll-2012-flat/dev-all/all.v4_auto_conll nn_coref/nn/conllouts/dev.bps.out nn_coref/nn/conllouts/dev.bps.antecedents'.split(' ')
reference = corpora.Corpus.from_file("reference", open(sys.argv[1]))
sys_predict = corpora.Corpus.from_file("system", open(sys.argv[2]))
sys_predict.read_antecedents(open(sys.argv[3]))
extractor = error_extractors.ErrorExtractor(reference, spanning_tree_algorithms.recall_accessibility, spanning_tree_algorithms.precision_system_output)
extractor.add_system(sys_predict)
errors = extractor.get_errors()
you_errors = errors.filter(lambda error: ' '.join(error[0].attributes['tokens']).lower() in ('you', 'your'))
len(you_errors['system']['precision_errors'])
you_errors.visualize('system')
```

And this is a list of documents that contains those errors. There's also a
[hack](https://github.com/smartschat/cort/issues/15#issuecomment-296984930) 
that makes only documents with errors visible.

```
(bc/cnn/00/cnn_0000); part 000
(bc/cnn/00/cnn_0000); part 003
(bc/cnn/00/cnn_0000); part 007
(bc/cnn/00/cnn_0000); part 010
(bc/cnn/00/cnn_0000); part 013
(bc/cnn/00/cnn_0000); part 016
(bc/cnn/00/cnn_0000); part 017
(bc/msnbc/00/msnbc_0000); part 002
(bc/msnbc/00/msnbc_0000); part 007
(bc/msnbc/00/msnbc_0000); part 008
(bc/msnbc/00/msnbc_0000); part 012
(bc/msnbc/00/msnbc_0000); part 016
(bc/msnbc/00/msnbc_0000); part 018
(bc/msnbc/00/msnbc_0000); part 020
(bc/phoenix/00/phoenix_0000); part 000
(bc/phoenix/00/phoenix_0000); part 001
(bc/phoenix/00/phoenix_0000); part 002
(bc/phoenix/00/phoenix_0000); part 003
(bc/phoenix/00/phoenix_0000); part 004
(bc/phoenix/00/phoenix_0000); part 005
(bc/phoenix/00/phoenix_0000); part 006
(bc/phoenix/00/phoenix_0000); part 007
(bn/cnn/02/cnn_0250); part 000
(bn/cnn/03/cnn_0320); part 000
(bn/mnb/00/mnb_0020); part 000
(bn/pri/00/pri_0020); part 000
(bn/pri/00/pri_0050); part 000
(pt/nt/40/nt_4010); part 000
(pt/nt/40/nt_4010); part 001
(pt/nt/40/nt_4020); part 001
(pt/nt/41/nt_4110); part 000
(pt/nt/41/nt_4110); part 001
(pt/nt/42/nt_4210); part 000
(pt/nt/42/nt_4210); part 001
(pt/nt/42/nt_4220); part 000
(pt/nt/43/nt_4310); part 000
(pt/nt/43/nt_4320); part 000
(pt/nt/44/nt_4410); part 000
(pt/nt/44/nt_4410); part 001
(pt/nt/44/nt_4420); part 001
(pt/nt/45/nt_4510); part 000
(pt/nt/47/nt_4710); part 000
(pt/nt/58/nt_5810); part 000
(tc/ch/00/ch_0010); part 001
(tc/ch/00/ch_0010); part 002
(tc/ch/00/ch_0020); part 002
(tc/ch/00/ch_0030); part 000
(tc/ch/00/ch_0030); part 001
(tc/ch/00/ch_0030); part 002
(tc/ch/00/ch_0040); part 000
(tc/ch/00/ch_0040); part 001
(tc/ch/00/ch_0050); part 000
(wb/a2e/00/a2e_0010); part 000
(wb/c2e/00/c2e_0040); part 000
(wb/eng/00/eng_0010); part 005
(wb/eng/00/eng_0010); part 006
```

Right now I have to go back to iSRL. The RANLP deadline is approaching very fast.

## Sat 29 Apr

Found some weird entries with "None" in ON5V:

```
bn/voa/00/voa_0035  s2_4    bring   {None=(s2_0,s2_1,s2_2,s2_3),Arg1=(s2_5,s2_6,s2_7),Arg2=(s2_8,s2_9,s2_10,s2_11,s2_12,s2_13,s2_14),Arg4+res=(s1_12)}
```

## Sun 30 Apr

Test these models:

```
ubuntu@packer-ubuntu-16:/data/isrl-sp$ grep "saved" run-neural-vm0.sh.out | uniq
Best model saved into out/bd79cd8999cee495a63047f4897bbdb81ff1d232/872642/coherence-model.npz
Best model saved into out/working-dir/356881/predicate-only.npz
Best model saved into out/working-dir/2892/synsem.npz
Best model saved into out/working-dir/95827552/predicate-with-synsem-features.npz
Best model saved into out/working-dir/1785262/coherence-with-synsem-features.npz
```

- cohrence-model: 0.14150943396226415
- predicate-only: 0.02358490566037736
- synsem: 0.018867924528301886
- predicate-with-synsem-features: 0.009433962264150943
- coherence-with-synsem-features: 0.10377358490566038

## Mon 1 May

Meeting with Piek and Antske:

- good news about EACL paper: feedback from Israel /ˈɪzɹeɪəl/ 
- failed attempt of an RANLP paper
- coreference resolution: hard cases of "you": pleonasm, new entities

## Wed 10 May

iSRL: testing the idea of resuming training on iSRL data. 
I implemented one experiment with the multi-way
model. Running now:

```
nohup python3 -u exp-coherence-with-semeval-data.py 3 &
```

## Thu 11 May

```
import cort
from cort.core import corpora
from cort.analysis import error_extractors
from cort.analysis import spanning_tree_algorithms
import sys
sys.argv = ' data/conll-2012-flat/dev-all/all.v4_auto_conll nn_coref/nn/conllouts/dev.bps.out nn_coref/nn/conllouts/dev.bps.antecedents'.split(' ')
reference = corpora.Corpus.from_file("reference", open(sys.argv[1]))
all_you = [(doc.identifier, i) for doc in reference.documents for i in range(len(doc.tokens)) if doc.tokens[i].lower() in ('you', 'your')]
ref_mention_you = set(m for doc in reference.documents for m in doc.annotated_mentions if m.attributes['tokens_as_lowercase_string'] in ('you', 'your'))
sys_mention_you = set(m for doc in sys_predict.documents for m in doc.annotated_mentions if m.attributes['tokens_as_lowercase_string'] in ('you', 'your'))
print('%d vs %d' %(len(ref_mention_you), len(sys_mention_you)))
print('True positive: %d' %len(ref_mention_you.intersection(sys_mention_you)))
print('False positive: %d' %len(sys_mention_you.difference(ref_mention_you)))
print('False negative: %d' %len(ref_mention_you.difference(sys_mention_you)))

854 vs 929
True positive: 817
False positive: 112
False negative: 37
--> P=0.8794, R=0.9567, F1=0.9164
```

Looking at fillers of A0, some curious cases struck me:

- diabetic complications --> frame: complication.01, A0: diabetic (an adjective).
A0 is defined as "agent, entity causing something to become complicated" so 
the actual filler is diabetes.
- Japanese competition: similar. The actual filler is Japan.

Compiled statistics about fillers of A0, A1 and A2.

Met with Piek and Antske. They think I was shooting a mosquito with a canon.
And I didn't tell them that coreference resolution of "you" has a F1 of 91% :))
They think I should do an analysis of coreference resolution instead and only if 
current systems demonstrate a lack of the knowledge that I want to model,
I will implement a new one. 

## Fri 12 May

(iSRL) Ran some statistics (`compare_semeval_and_ontonotes.py`). The coverage
is not bad so it's not that OntoNotes can't help here.

```
Frame with DNI: Unique=80, count=118
Overlap: 389
+OntoNotes, -SemEval: 5201
-OntoNotes, +SemEval(train+test): 143
-OntoNotes, +SemEval(test): 89 (count=1674)
-OntoNotes, +SemEval(test,DNI): 13 (count=16)
```

## Mon 15 May

I started to think that my whole PhD was based on false premises. The whole
problem was with the world "knowledge". There was 
[so many kinds of knowledge](http://natural-language-understanding.wikia.com/wiki/Knowledge#Classification)
so I have to choose. I tried to model lexical knowledge before for entity linking
but it didn't help (might be I did it in a bad way since after I gave up 
there was some papers published about entity/sense embeddings).
More recently, I chose to model event chain and co-participation. Event chains were done
before (though might not be in the best way) and co-participation might not
make much difference. What if the knowledge that matters is something else?

I don't have time to ponder this anymore. I need to get this analysis done to 
get my PhD. Besides, I have many machine learning ideas that I want to turn
into B/C publications but I'll have to do them out of office hours.

## Fri 19 May

Working on gender manipulation:

This will be a litter counter-intuitive for humans:

    The ``USS Cole'' has arrived back at the Mississippi shipyard where he was built. 

People might expect the ship is a "she" but since we turn everything into a "he"
so "he" it is.

And this will become "father and father":

    Those who love their father or mother more than they love me are not worthy of me .

Talked to Antske, she reminded me again that I should have a plan and think
everything through before starting playing around. OK... I have rough ideas
about what each manipulation will do to each kind of systems. I think they are
too trivial to write down but since she insists on it, well... 

Tried to write things down and surprisingly the gender idea doesn't fit. The
most natural way to test my hypothesis is to 

## Monday 22 May

neural net coref, events masked, predicted mentions:

```
[main] INFO CoreNLP - Identification of Mentions: Recall: (16628 / 18577) 89.5% Precision: (16628 / 42343) 39.26%   F1: 54.58%
[main] INFO CoreNLP - METRIC muc:Coreference: Recall: (7514 / 14074) 53.38% Precision: (7514 / 13829) 54.33%    F1: 53.85%
METRIC bcub:Coreference: Recall: (6768.29 / 18577) 36.43%   Precision: (9464.73 / 17463) 54.19% F1: 43.57%
METRIC ceafm:Coreference: Recall: (8355 / 18577) 44.97% Precision: (8355 / 17463) 47.84%    F1: 46.36%
METRIC ceafe:Coreference: Recall: (1766.73 / 4503) 39.23%   Precision: (1766.73 / 3634) 48.61%  F1: 43.42%
METRIC blanc:Coreference links: Recall: (36073 / 95041) 37.95%  Precision: (36073 / 129355) 27.88%  F1: 32.15%
Non-coreference links: Recall: (280766 / 792411) 35.43% Precision: (280766 / 651330) 43.1%  F1: 38.89%
BLANC: Recall: (0.37 / 1) 36.69%    Precision: (0.35 / 1) 35.49%    F1: 35.52%
[main] INFO CoreNLP - Final conll score ((muc+bcub+ceafe)/3) = 46.95
```

neural net coref, mentions masked, predicted mentions:

```
[main] INFO CoreNLP - Identification of Mentions: Recall: (16606 / 18585) 89.35%    Precision: (16606 / 41517) 39.99%   F1: 55.25%
[main] INFO CoreNLP - METRIC muc:Coreference: Recall: (5531 / 14078) 39.28% Precision: (5531 / 10172) 54.37%    F1: 45.61%
METRIC bcub:Coreference: Recall: (5026.75 / 18585) 27.04%   Precision: (4650.56 / 12754) 36.46% F1: 31.05%
METRIC ceafm:Coreference: Recall: (4789 / 18585) 25.76% Precision: (4789 / 12754) 37.54%    F1: 30.56%
METRIC ceafe:Coreference: Recall: (872.32 / 4507) 19.35%    Precision: (872.32 / 2582) 33.78%   F1: 24.61%
METRIC blanc:Coreference links: Recall: (19502 / 95046) 20.51%  Precision: (19502 / 88740) 21.97%   F1: 21.22%
Non-coreference links: Recall: (202205 / 792466) 25.51% Precision: (202205 / 344876) 58.63% F1: 35.55%
BLANC: Recall: (0.23 / 1) 23.01%    Precision: (0.4 / 1) 40.3%  F1: 28.38%
[main] INFO CoreNLP - Final conll score ((muc+bcub+ceafe)/3) = 33.76
```

## Mon 29 May

New results using gold mentions:

sieve, event masked, gold mentions, intact gold parses:

```
INFO: METRIC muc:Coreference: Recall: (10325 / 14086) 73.29%    Precision: (10325 / 11397) 90.59%   F1: 81.03%
METRIC bcub:Coreference: Recall: (11043.23 / 18597) 59.38%  Precision: (12876.17 / 14957) 86.08%    F1: 70.28%
METRIC ceafm:Coreference: Recall: (11962 / 18597) 64.32%    Precision: (11962 / 14957) 79.97%   F1: 71.29%
METRIC ceafe:Coreference: Recall: (2655.91 / 4511) 58.87%   Precision: (2655.91 / 3560) 74.6%   F1: 65.81%
METRIC blanc:Coreference links: Recall: (55253 / 95060) 58.12%  Precision: (55253 / 66562) 83%  F1: 68.37%
Non-coreference links: Recall: (519560 / 792662) 65.54% Precision: (519560 / 547654) 94.87% F1: 77.52%
BLANC: Recall: (0.62 / 1) 61.83%    Precision: (0.89 / 1) 88.93%    F1: 72.95%
```

Re-structuring the codebase: remove copy-paste-modify code, move utility scripts
into scripts, only keeping experiments under the root folder.

Problem: the `test` data has automatic parse ([auto_conll](http://natural-language-understanding.wikia.com/wiki/CoNLL-2011_shared_task)) 
but no gold mentions while the `test-key` data has gold mention but
no automatic parse (only gold parse).

## Wed 31 May

Twitched neural coref to run on gold mentions and gold parses.

## Thu 1 Jun

Mask syntax, propositions and named entities as well.

nn, mention-masked, masked gold parses, gold mentions

```
[main] INFO CoreNLP - Identification of Mentions: Recall: (18586 / 18586) 100%  Precision: (18586 / 18588) 99.98%   F1: 99.99%
[main] INFO CoreNLP - METRIC muc:Coreference: Recall: (663 / 14079) 4.7%    Precision: (663 / 1270) 52.2%   F1: 8.63%
METRIC bcub:Coreference: Recall: (671.7 / 18586) 3.61%  Precision: (772.34 / 1675) 46.1%    F1: 6.7%
METRIC ceafm:Coreference: Recall: (855 / 18586) 4.6%    Precision: (855 / 1675) 51.04%  F1: 8.43%
METRIC ceafe:Coreference: Recall: (170.78 / 4507) 3.78% Precision: (170.78 / 405) 42.16%    F1: 6.95%
METRIC blanc:Coreference links: Recall: (1577 / 95047) 1.65%    Precision: (1577 / 5543) 28.45% F1: 3.13%
Non-coreference links: Recall: (3920 / 792477) 0.49%    Precision: (3920 / 4863) 80.6%  F1: 0.98%
BLANC: Recall: (0.01 / 1) 1.07% Precision: (0.55 / 1) 54.52%    F1: 2.05%
[main] INFO CoreNLP - Final conll score ((muc+bcub+ceafe)/3) = 7.43
```

sieve, mention-masked, masked gold parses, gold mentions

```
INFO: METRIC muc:Coreference: Recall: (9307 / 14086) 66.07% Precision: (9307 / 12757) 72.95%    F1: 69.34%
METRIC bcub:Coreference: Recall: (12189.11 / 18597) 65.54%  Precision: (1773.52 / 13087) 13.55% F1: 22.45%
METRIC ceafm:Coreference: Recall: (3167 / 18597) 17.02% Precision: (3167 / 13087) 24.19%    F1: 19.99%
METRIC ceafe:Coreference: Recall: (142.9 / 4511) 3.16%  Precision: (142.9 / 330) 43.3%  F1: 5.9%
METRIC blanc:Coreference links: Recall: (48573 / 95060) 51.09%  Precision: (48573 / 517766) 9.38%   F1: 15.85%
Non-coreference links: Recall: (28913 / 792662) 3.64%   Precision: (28913 / 30952) 93.41%   F1: 7.02%
BLANC: Recall: (0.27 / 1) 27.37%    Precision: (0.51 / 1) 51.39%    F1: 11.43%
```

## Fri 2 Jun

Talked to Antske, she thinks automatic parses and NERs are still better for the
paper since people care more about realistic performance. Actually, with my
most recent implementation, masking incomplete subtrees isn't a problem.

## Thu 8 Jun

About the question of gold/predicted input: Bengtson & Roth (2008) use gold mention, 
Recasens & Hovy (2009) use gold morphological, syntactic and semantic information.

- Bengtson, E., & Roth, D. (2008). Understanding the value of features for coreference resolution. Proceedings of the Conference on Empirical Methods in Natural Language Processing - EMNLP ’08, 51(October), 294. http://doi.org/10.3115/1613715.1613756
- Recasens, M., & Hovy, E. (2009). A deeper look into features for coreference resolution. Lecture Notes in Computer Science (Including Subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics), 5847 LNAI(i), 29–42. http://doi.org/10.1007/978-3-642-04975-0_3

## Thu 15 Jun

[Met Piek and Antske](https://docs.google.com/document/d/1EPrOlPj4LmViKHhFw-o-NGUbanE2Hu7fLXcvRzxijfU/edit)

## Mon 19 Jun

Make small progress with manipulations: retaining only mentions and shuffling 
them, replacing names with identifiers.

Apparently, I'll need to retrain models and add new entries to gazetteers
to make use of those identifiers.

## Wed 21 Jun

I tried to create names that are nice for people (e.g. George Bush -> 
MaleName1 FamilyName1) but there are so many special cases. The annotation
doesn't allow to know where a person's name starts (e.g. President Obama can
be treated as if President is the first name). Geo-political name is usually 
attached to facility names (e.g. Hong Kong Disneyland). So perhaps the best
thing to do is just replace any NNP (regardless of mentions and NER labels),
by a name identifier. For humans, we can show NER labels separately (but we're
not going to show this to humans anyway). 

## Thu 22 Jun

Decided to map all male-sounding name to MName, female-sounding names FName
and plural-sounding names PName.

Problem detected: training Stanford NN model takes 7 days. I'll need to train
at least 6 of them! (And it's very likely that I'll need to retrain everything
several times.)

What I've more or less finished:

- mention
- mention+position
- mention+position+instance

What I'll do tomorrow:

- mention+position+instance+syntax
- write the part about syntax features
- write Cartesius proposal

## Fri 23 Jun

Interesting case in a2e\_0000\_part\_000: 

> "An elderly man said , " I asked them not to shoot and asked them for God 's sake not to do it , but they persisted in it , until what happened happened ."

## Sun 25 Jun

Studying the output of nn_coref again.

Figured out how to remove uninteresting mention boundary errors:

```
def remove_mention_boundary_errors(errors, ref_corpus, sys_corpus):
    ref_doc2spans = dict((doc.identifier, get_all_mention_spans(doc)) 
                         for doc in ref_corpus.documents)
    sys_doc2spans = dict((doc.identifier, get_all_mention_spans(doc)) 
                         for doc in sys_corpus.documents)
    precision_errors = errors["system"]["precision_errors"].filter(lambda e:
            e[0].span in ref_doc2spans[e[0].document.identifier] and
            e[1].span in ref_doc2spans[e[1].document.identifier])
    recall_errors = errors["system"]["recall_errors"].filter(lambda e:
            e[0].span in sys_doc2spans[e[0].document.identifier] and
            e[1].span in sys_doc2spans[e[1].document.identifier])
    mapping = {'system': {'recall_errors': recall_errors,
                          'precision_errors': precision_errors,
                          'decisions': errors['system']['decisions']}}
    return StructuredCoreferenceAnalysis(mapping, errors.corpora, errors.reference) 
```

Immediately, I found out cases where one must use events to disambiguate:

1. `a2e_0000_part_000`: "one of the celebrators lost control of **his** weapon , from which a number of bullets were released that killed **the groom 's brother**": the system incorrectly links two bold spans together.
2. same doc: "**one of the celebrators** lost control of his weapon [some sentences in between] **one of the armed men** lost control of his weapon": the system misses this link
3. same doc: "from which a number of bullets were released that killed **the groom 's brother** ... **Palestinian citizen Kamal Mohamed Al Bashiti -LRB- 33 years old -RRB-** died shortly after he was hit by a gunshot in the chest from": misses again
4. `a2e_0020_part_001`: "His survival would have benefited it much more than his execution if **they** understood politics as they should , because his survival could have been a card to threaten **the sectarians** and keep **them** as servants to **them**": the system links both *them* to *the sectarians* which doesn't make sense because they would be servants to themselves
5. `a2e_0030_part_000`: "The two students , Alexander Ghuliyandrin and Oleg Darutsigh , said that they were on a trip last Saturday morning when they reached a military fortification near Qisrin . [some sentences] the descriptions of the place and the fortification given by the two boys"
6. `abc_0010_part_000`: "The ship began to take on water and lost electrical power again . **It** led to the cancellation of a memorial service planned for the 17 sailors who lost their lives ." -> *it* was incorrectly linked to *the ship* instead of *began*.
7. `abc_0020_part_000`: "**They** were the last of the dead to be flown home , but in Yemen , the search for those responsible for the bombing continues . This house on Al Zaharah Street half a mile from the port is where investigators believe the bomb was built into the boat that carried it . Just after **the men** moved in , neighbors say they erected a metal corrugated wall to block the neighbors ' view of the yard ." -> this one is more subtle: the mention of *house* is related to *moved in*. The correct solution is not to link *the men* to anything before it. The system incorrectly links it to previous *they*.
8. `cctv_0000_part_000`: "The world 's fifth **Disney park** will soon open to the public here . The most important thing about **Disney** is that it is a global brand . Well , for several years , although **it** was still under construction": system links *it* to *Disney*, correct solution is *Disney park*.
9. `cctv_0000_part_000`: "The subway to Disney has already been constructed .": *Disney* here should **not** be linked to other *Disney* but to *Disneyland*.   


## Wed 28 Jun 2017

Started using BitBucket's continuous integration service. It surely generates a 
sense of satisfaction to see that [your code builds](https://bitbucket.org/cltl/even/addon/pipelines/home#!/results/%7B15b31574-34fb-4727-b410-7e97efe92ad8%7D).

## Thu 29 Jun 2017

Kind of finished manipulations. I need to be extra careful because finding a
problem after annotators did their job would be a disaster.

Now I'll need to create Brat files (or whatever annotation tool it is).
Maybe next week >"<

## Wed 5 July 2017

Finished the code to create brat files. I annotated one of them [here](bn_cnn_01_cnn_0190___part_000.ann). I wanted to present today before CLTL 
meeting but Tommaso forgot me :-( I'll try it next week then.

Now it's time to train and run systems on manipulated corpora. That reminds me, 
I didn't finish Cartesius application :-( 

## Mon 10 July 2017

Finished, submitted Cartesius application.

Have another productive meeting with Piek. He agrees with me about a PhD thesis
on error propagation. Yeah!

idea: possible criteria for choosing documents
- number of mentions that don't string-match
- number of mentions in a chain (longer chains imply events are more important???)
- number of chains
- performance is kind of low
- inter-annotator agreement on that part

## Tue 11 July

Have some problem with running coreferencers against the modulated corpora.
It is because of incorrect handling of NER spans when part of them is removed.

## Wed 12 July

Solved the problem for mention, mention+position, mention+position+instance but
still having it with syntax.

CLTL presentation: people liked it a lot. I also have a lot of feedback.

## Thu 13 July

Tried Phrase Detectives and summarized my thoughts into 
[a blog post](https://minhlab.wordpress.com/2017/07/14/phrase-detectives-caught-me-by-surprise/).

Talked to Antske:

- uncertainty --> people use different strategy
- high inter-annotator agreement --> same strategy? --> not necessarily good
- train people specifically on the tasks
- crowd sourcing doesn't seem as bad as I thought --> do a pilot study?

## Fri 14 July

Come back to running systems on different versions of CoNLL-2012.

At least one of them finished successfully:

```
$ scripts/run-nn-coref-auto.sh /Users/cumeo/Projects/spinoza/ulm-4/EvEn/data/conll-2012-manipulated/mentions-dev
...
[main] INFO CoreNLP - Identification of Mentions: Recall: (18616 / 18616) 100%  Precision: (18616 / 18633) 99.9%    F1: 99.95%
[main] INFO CoreNLP - METRIC muc:Coreference: Recall: (4665 / 14109) 33.06% Precision: (4665 / 5510) 84.66% F1: 47.55%
METRIC bcub:Coreference: Recall: (4062.86 / 18616) 21.82%   Precision: (6150.17 / 7656) 80.33%  F1: 34.32%
METRIC ceafm:Coreference: Recall: (5249 / 18616) 28.19% Precision: (5249 / 7656) 68.56% F1: 39.95%
METRIC ceafe:Coreference: Recall: (1101.59 / 4507) 24.44%   Precision: (1101.59 / 2146) 51.33%  F1: 33.11%
METRIC blanc:Coreference links: Recall: (24077 / 95626) 25.17%  Precision: (24077 / 36417) 66.11%   F1: 36.46%
Non-coreference links: Recall: (109188 / 796144) 13.71% Precision: (109188 / 128080) 85.24% F1: 23.62%
BLANC: Recall: (0.19 / 1) 19.44%    Precision: (0.76 / 1) 75.68%    F1: 30.04%
[main] INFO CoreNLP - Final conll score ((muc+bcub+ceafe)/3) = 38.33
```

Another one:

```
$ scripts/run-sieve-auto.sh /Users/cumeo/Projects/spinoza/ulm-4/EvEn/data/conll-2012-manipulated/mentions-dev
CONLL EVAL SUMMARY (Before COREF)
Jul 14, 2017 2:47:11 PM edu.stanford.nlp.dcoref.SieveCoreferenceSystem printScoreSummary
INFO: Identification of Mentions: Recall: (18762 / 18762) 100%  Precision: (18762 / 18762) 100% F1: 100%
Jul 14, 2017 2:47:24 PM edu.stanford.nlp.dcoref.SieveCoreferenceSystem runAndScoreCoref
INFO:
CONLL EVAL SUMMARY (After COREF)
Jul 14, 2017 2:47:24 PM edu.stanford.nlp.dcoref.SieveCoreferenceSystem printScoreSummary
INFO: METRIC muc:Coreference: Recall: (6396 / 14219) 44.98% Precision: (6396 / 8819) 72.52% F1: 55.52%
METRIC bcub:Coreference: Recall: (6437.8 / 18762) 34.31%    Precision: (8436.16 / 11863) 71.11% F1: 46.29%
METRIC ceafm:Coreference: Recall: (7742 / 18762) 41.26% Precision: (7742 / 11863) 65.26%    F1: 50.55%
METRIC ceafe:Coreference: Recall: (1613.47 / 4543) 35.51%   Precision: (1613.47 / 3044) 53% F1: 42.53%
METRIC blanc:Coreference links: Recall: (36772 / 96236) 38.21%  Precision: (36772 / 49659) 74.04%   F1: 50.4%
Non-coreference links: Recall: (301497 / 803290) 37.53% Precision: (301497 / 334922) 90.02% F1: 52.97%
BLANC: Recall: (0.38 / 1) 37.87%    Precision: (0.82 / 1) 82.03%    F1: 51.69%

Jul 14, 2017 2:47:24 PM edu.stanford.nlp.dcoref.SieveCoreferenceSystem printFinalConllScore
INFO: Final conll score ((muc+bcub+ceafe)/3) = 48.11
Jul 14, 2017 2:47:24 PM edu.stanford.nlp.dcoref.SieveCoreferenceSystem getFinalScore
INFO: Final score (pairwise) Precision = 0.74
Jul 14, 2017 2:47:24 PM edu.stanford.nlp.dcoref.SieveCoreferenceSystem initializeAndRunCoref
INFO: done
```

It seems that some problems I have before with running the systems is caused
by having too many of them running in parallel. I'm trying to run them again
one by one.

## Sat 15 Jul

I was right. They all ran successfully when each one ran alone.

Fixed the problem with syntax version. There were two bugs:

- I forgot to normalize NER layer for mentions
- I forgot to erase NER layer of empty sentences (those contain no mentions)

Restarted the experiments on DAS-5 (login node):

```
[minhle@fs0 EvEn]$ tail -f nohup.out
Reading data/conll-2012-flat/train/bn_abc_0003.v4_auto_conll... Done.
Reading data/conll-2012-flat/train/bn_abc_0062.v4_auto_conll... Done.
Reading data/conll-2012-flat/train/bn_abc_0004.v4_auto_conll... Done.
Reading data/conll-2012-flat/train/bn_abc_0063.v4_auto_conll... Done.
...
```

## Mon 17 Jul

Implemented genre code. I'm surprised that there's no big variance among genres
in terms of performance. 

```
    stanford_sieve      stanford_nn
    origin  syntax  origin  syntax
bc  72.54   72.40   71.81   67.11
bn  68.37   68.63   76.16   71.20
mz  73.63   72.63   80.51   74.81
nw  72.18   71.55   76.63   70.03
pt  72.83   72.06   83.44   74.51
tc  78.08   78.54   76.86   73.01
wb  71.65   71.60   73.79   68.09
```

## Tue 18 July

Finished [some statistics](https://docs.google.com/spreadsheets/d/1GJqqN9noljOSp5CSEHNnxBee4M33-PFB33zxeFn5-og/edit?ts=596dbe4b#gid=552426196) 
and sent it to Piek and Antske. I think broadcast conversation is the best choice.

Now I need to run nn_coref against the corpora. Academic software is such a mess!
Let's do it in baby steps:

1. Run pre-trained model, pre-extract features to see if we get reasonable results
2. Extract features of original datasets and run pre-trained model, check if we get the same reuslts
3. Extract features of modulated datasets and run pre-trained model
4. Train new models on modulated train datasets and evaluate on modulated dev datasets 

These steps might well take 3-4 days!!! I'll do it in parallel with developing
some new models because I can't bear the boredom!!

